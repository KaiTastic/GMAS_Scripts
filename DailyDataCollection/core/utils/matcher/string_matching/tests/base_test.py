#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•åŸºç±» - ä¸ºæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æä¾›å…¬å…±åŠŸèƒ½
"""

import unittest
import time
import traceback
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass
from enum import Enum
import json


class TestResult(Enum):
    """æµ‹è¯•ç»“æžœæžšä¸¾"""
    PASS = "PASS"
    FAIL = "FAIL"
    ERROR = "ERROR"
    SKIP = "SKIP"


@dataclass
class TestCaseResult:
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹ç»“æžœ"""
    test_name: str
    result: TestResult
    execution_time: float
    error_message: Optional[str] = None
    details: Optional[Dict[str, Any]] = None
    expected: Optional[Any] = None
    actual: Optional[Any] = None


class BaseTestCase(unittest.TestCase):
    """æµ‹è¯•åŸºç±»
    
    æä¾›é€šç”¨çš„æµ‹è¯•åŠŸèƒ½å’Œæ–­è¨€æ–¹æ³•
    """
    
    def __init__(self, methodName='runTest'):
        super().__init__(methodName)
        self.test_results: List[TestCaseResult] = []
        self.start_time = None
        self.end_time = None
        
    def setUp(self):
        """æµ‹è¯•å‰è®¾ç½®"""
        self.start_time = time.time()
        
    def tearDown(self):
        """æµ‹è¯•åŽæ¸…ç†"""
        self.end_time = time.time()
        
    def run_test_case(self, test_name: str, test_func: Callable, 
                      expected_result: Any = None, **kwargs) -> TestCaseResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹
        
        Args:
            test_name: æµ‹è¯•ç”¨ä¾‹åç§°
            test_func: æµ‹è¯•å‡½æ•°
            expected_result: æœŸæœ›ç»“æžœ
            **kwargs: æµ‹è¯•å‡½æ•°å‚æ•°
            
        Returns:
            TestCaseResult: æµ‹è¯•ç»“æžœ
        """
        start_time = time.time()
        result = TestResult.PASS
        error_message = None
        actual_result = None
        details = {}
        
        try:
            # æ‰§è¡Œæµ‹è¯•å‡½æ•°
            actual_result = test_func(**kwargs)
            
            # å¦‚æžœæä¾›äº†æœŸæœ›ç»“æžœï¼Œè¿›è¡Œæ¯”è¾ƒ
            if expected_result is not None:
                if actual_result != expected_result:
                    result = TestResult.FAIL
                    error_message = f"æœŸæœ›ç»“æžœ: {expected_result}, å®žé™…ç»“æžœ: {actual_result}"
                    
        except AssertionError as e:
            result = TestResult.FAIL
            error_message = str(e)
            
        except Exception as e:
            result = TestResult.ERROR
            error_message = f"{type(e).__name__}: {str(e)}"
            details['traceback'] = traceback.format_exc()
            
        execution_time = time.time() - start_time
        
        test_result = TestCaseResult(
            test_name=test_name,
            result=result,
            execution_time=execution_time,
            error_message=error_message,
            details=details,
            expected=expected_result,
            actual=actual_result
        )
        
        self.test_results.append(test_result)
        return test_result
    
    def assert_fuzzy_match(self, actual: float, expected: float, 
                          tolerance: float = 0.01, message: str = ""):
        """æ–­è¨€æ¨¡ç³ŠåŒ¹é…åˆ†æ•°
        
        Args:
            actual: å®žé™…åˆ†æ•°
            expected: æœŸæœ›åˆ†æ•°
            tolerance: å®¹å·®
            message: é”™è¯¯æ¶ˆæ¯
        """
        if abs(actual - expected) > tolerance:
            fail_msg = f"åˆ†æ•°å·®å¼‚è¿‡å¤§: æœŸæœ› {expected}, å®žé™… {actual}, å®¹å·® {tolerance}"
            if message:
                fail_msg = f"{message}: {fail_msg}"
            self.fail(fail_msg)
            
    def assert_match_result(self, result, expected_matches: Dict[str, str], 
                           min_score: float = 0.5):
        """æ–­è¨€åŒ¹é…ç»“æžœ
        
        Args:
            result: åŒ¹é…ç»“æžœå¯¹è±¡
            expected_matches: æœŸæœ›çš„åŒ¹é…å­—å…¸
            min_score: æœ€å°åˆ†æ•°è¦æ±‚
        """
        # æ£€æŸ¥æ€»åˆ†æ•°
        self.assertGreaterEqual(result.overall_score, min_score, 
                               f"æ•´ä½“åˆ†æ•°è¿‡ä½Ž: {result.overall_score}")
        
        # æ£€æŸ¥æœŸæœ›çš„åŒ¹é…é¡¹
        for target_name, expected_value in expected_matches.items():
            actual_value = result.get_matched_value(target_name)
            self.assertEqual(actual_value, expected_value, 
                           f"ç›®æ ‡ {target_name} åŒ¹é…é”™è¯¯: æœŸæœ› {expected_value}, å®žé™… {actual_value}")
    
    def assert_target_config(self, config, expected_type, expected_patterns: List[str]):
        """æ–­è¨€ç›®æ ‡é…ç½®
        
        Args:
            config: ç›®æ ‡é…ç½®å¯¹è±¡
            expected_type: æœŸæœ›çš„ç›®æ ‡ç±»åž‹
            expected_patterns: æœŸæœ›çš„æ¨¡å¼åˆ—è¡¨
        """
        self.assertEqual(config.target_type, expected_type, 
                        f"ç›®æ ‡ç±»åž‹é”™è¯¯: æœŸæœ› {expected_type}, å®žé™… {config.target_type}")
        
        for pattern in expected_patterns:
            self.assertIn(pattern, config.patterns, 
                         f"æ¨¡å¼ {pattern} ä¸åœ¨é…ç½®ä¸­")
    
    def get_test_summary(self) -> Dict[str, Any]:
        """èŽ·å–æµ‹è¯•æ€»ç»“
        
        Returns:
            Dict[str, Any]: æµ‹è¯•æ€»ç»“ä¿¡æ¯
        """
        total_tests = len(self.test_results)
        if total_tests == 0:
            return {"total": 0, "summary": "æ— æµ‹è¯•ç”¨ä¾‹"}
            
        pass_count = sum(1 for r in self.test_results if r.result == TestResult.PASS)
        fail_count = sum(1 for r in self.test_results if r.result == TestResult.FAIL)
        error_count = sum(1 for r in self.test_results if r.result == TestResult.ERROR)
        skip_count = sum(1 for r in self.test_results if r.result == TestResult.SKIP)
        
        total_time = sum(r.execution_time for r in self.test_results)
        avg_time = total_time / total_tests if total_tests > 0 else 0
        
        return {
            "total": total_tests,
            "pass": pass_count,
            "fail": fail_count,
            "error": error_count,
            "skip": skip_count,
            "pass_rate": (pass_count / total_tests) * 100,
            "total_time": total_time,
            "average_time": avg_time,
            "details": self.test_results
        }
    
    def print_test_results(self):
        """æ‰“å°æµ‹è¯•ç»“æžœ"""
        summary = self.get_test_summary()
        
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•æ€»ç»“ - {self.__class__.__name__}")
        print(f"{'='*60}")
        print(f"æ€»ç”¨ä¾‹æ•°: {summary['total']}")
        print(f"é€šè¿‡: {summary['pass']} | å¤±è´¥: {summary['fail']} | é”™è¯¯: {summary['error']} | è·³è¿‡: {summary['skip']}")
        print(f"é€šè¿‡çŽ‡: {summary['pass_rate']:.1f}%")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {summary['total_time']:.4f}ç§’")
        print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {summary['average_time']:.4f}ç§’")
        
        print(f"\nè¯¦ç»†ç»“æžœ:")
        print(f"{'-'*60}")
        
        for i, result in enumerate(self.test_results, 1):
            status_icon = {
                TestResult.PASS: "âœ…",
                TestResult.FAIL: "âŒ", 
                TestResult.ERROR: "ðŸ’¥",
                TestResult.SKIP: "â­ï¸"
            }.get(result.result, "â“")
            
            print(f"{i:2d}. {status_icon} {result.test_name} ({result.execution_time:.4f}s)")
            
            if result.error_message:
                print(f"    é”™è¯¯: {result.error_message}")
                
            if result.expected is not None and result.actual is not None:
                print(f"    æœŸæœ›: {result.expected}")
                print(f"    å®žé™…: {result.actual}")
                
        print(f"{'='*60}\n")
    
    def save_test_results(self, filepath: str):
        """ä¿å­˜æµ‹è¯•ç»“æžœåˆ°æ–‡ä»¶
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
        """
        summary = self.get_test_summary()
        
        # è½¬æ¢ç»“æžœä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_results = []
        for result in self.test_results:
            serializable_results.append({
                "test_name": result.test_name,
                "result": result.result.value,
                "execution_time": result.execution_time,
                "error_message": result.error_message,
                "details": result.details,
                "expected": str(result.expected) if result.expected is not None else None,
                "actual": str(result.actual) if result.actual is not None else None
            })
        
        summary["details"] = serializable_results
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
            
        print(f"æµ‹è¯•ç»“æžœå·²ä¿å­˜åˆ°: {filepath}")


class PerformanceTestCase(BaseTestCase):
    """æ€§èƒ½æµ‹è¯•åŸºç±»"""
    
    def __init__(self, methodName='runTest'):
        super().__init__(methodName)
        self.performance_thresholds = {
            'execution_time': 1.0,  # ç§’
            'memory_usage': 100,    # MB
            'accuracy': 0.8         # å‡†ç¡®çŽ‡
        }
    
    def benchmark_function(self, func: Callable, iterations: int = 100, 
                          **kwargs) -> Dict[str, float]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            func: è¦æµ‹è¯•çš„å‡½æ•°
            iterations: è¿­ä»£æ¬¡æ•°
            **kwargs: å‡½æ•°å‚æ•°
            
        Returns:
            Dict[str, float]: æ€§èƒ½æŒ‡æ ‡
        """
        times = []
        
        for _ in range(iterations):
            start_time = time.time()
            func(**kwargs)
            end_time = time.time()
            times.append(end_time - start_time)
        
        return {
            'min_time': min(times),
            'max_time': max(times),
            'avg_time': sum(times) / len(times),
            'total_time': sum(times),
            'iterations': iterations
        }
    
    def assert_performance(self, metrics: Dict[str, float], 
                          max_avg_time: float = None):
        """æ–­è¨€æ€§èƒ½æŒ‡æ ‡
        
        Args:
            metrics: æ€§èƒ½æŒ‡æ ‡
            max_avg_time: æœ€å¤§å¹³å‡æ—¶é—´
        """
        if max_avg_time is not None:
            self.assertLessEqual(metrics['avg_time'], max_avg_time,
                               f"å¹³å‡æ‰§è¡Œæ—¶é—´è¿‡é•¿: {metrics['avg_time']:.4f}s > {max_avg_time}s")
