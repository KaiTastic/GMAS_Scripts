#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“é—¨çš„æµ‹è¯•æ•°æ®é›†å’Œæ•°æ®é©±åŠ¨æµ‹è¯•
åŒ…å«å„ç§çœŸå®åœºæ™¯çš„æµ‹è¯•æ•°æ®å’Œå¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹
"""

import unittest
import sys
import os
import json
from typing import List, Dict, Any, Tuple
import random

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

# å¯¼å…¥è¦æµ‹è¯•çš„æ¨¡å—
from exact_matcher import ExactStringMatcher
from fuzzy_matcher import FuzzyStringMatcher
from hybrid_matcher import HybridStringMatcher
from similarity_calculator import SimilarityCalculator


class TestDatasets:
    """æµ‹è¯•æ•°æ®é›†åˆç±»"""
    
    # å¤šè¯­è¨€äººåæ•°æ®é›†
    MULTILINGUAL_NAMES = {
        'english': [
            "John Smith", "Mary Johnson", "David Wilson", "Sarah Brown", "Michael Davis",
            "Jennifer Garcia", "Christopher Martinez", "Jessica Rodriguez", "Matthew Anderson", 
            "Amanda Taylor", "Daniel Moore", "Michelle Martin", "Anthony Jackson", "Lisa White"
        ],
        'chinese': [
            "å¼ ä¼Ÿ", "ç‹èŠ³", "æå¨œ", "åˆ˜å¼º", "é™ˆæ•", "æ¨é™", "èµµç£Š", "é»„ä¸½", "å‘¨å‹‡", "å´è‰³",
            "å¾æ˜", "å­™æ´", "æœ±å†›", "é©¬çº¢", "èƒ¡æ¶›", "éƒ­è‰", "æ—å³°", "ä½•ç¾", "é«˜é¹", "æ¢éœ"
        ],
        'japanese': [
            "ç”°ä¸­å¤ªéƒ", "ä½è—¤èŠ±å­", "é«˜æ©‹ä¸€éƒ", "å±±ç”°ç¾å’²", "æ¸¡è¾ºå¥å¤ª", "ä¼Šè—¤ã‚ã„",
            "ä¸­æ‘å¤§è¼”", "å°æ—çœŸç†", "åŠ è—¤ç¿”å¤ª", "å‰ç”°ã¿ã‚†ã", "å±±å£éš†", "æ¾æœ¬èˆ",
            "äº•ä¸Šæ‹“ä¹Ÿ", "æœ¨æ‘æµå­", "æ–è—¤é›„ä»‹", "æ¸…æ°´ç”±ç¾å­"
        ],
        'korean': [
            "ê¹€ë¯¼ìˆ˜", "ì´ì§€ì˜", "ë°•ì¤€í˜¸", "ìµœìˆ˜ì§„", "ì •ìš°ì„", "í•œì†Œì˜", "ì„ëŒ€í•œ", "ì†¡ë¯¼ê²½",
            "ìœ¤ì¬í˜„", "ê°•ì€ì§€", "ì¡°ì„±ë¯¼", "ì‹ í˜œì§„", "ì¥ë™ìš±", "ì˜¤ìˆ˜ì •", "í™ì§„ìš°", "ë¬¸ì§€ì€"
        ],
        'arabic': [
            "Ù…Ø­Ù…Ø¯ Ø£Ø­Ù…Ø¯", "ÙØ§Ø·Ù…Ø© Ø¹Ù„ÙŠ", "Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø­Ø³Ù†", "Ø¹Ø§Ø¦Ø´Ø© Ù…Ø­Ù…ÙˆØ¯", "Ø¹Ù…Ø± Ø³Ø§Ù„Ù…",
            "Ø®Ø¯ÙŠØ¬Ø© ÙŠÙˆØ³Ù", "Ø­Ø³Ø§Ù… Ø§Ù„Ø¯ÙŠÙ†", "Ù†ÙˆØ±Ø§ Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ…", "Ø·Ø§Ø±Ù‚ Ø¹Ø¨Ø¯Ø§Ù„Ø±Ø­Ù…Ù†", "Ù…Ø±ÙŠÙ… Ø®Ø§Ù„Ø¯"
        ]
    }
    
    # ç§‘æŠ€äº§å“æ•°æ®é›†
    TECH_PRODUCTS = {
        'smartphones': [
            "iPhone 15 Pro Max 1TB Natural Titanium",
            "Samsung Galaxy S24 Ultra 1TB Titanium Gray",
            "Google Pixel 8 Pro 512GB Obsidian",
            "OnePlus 12 512GB Flowy Emerald", 
            "Xiaomi 14 Ultra 1TB Photography Kit",
            "Sony Xperia 1 V 512GB Platinum Silver",
            "ASUS ROG Phone 8 Pro 1TB Phantom Black",
            "Nothing Phone (2a) 256GB Milk White"
        ],
        'laptops': [
            "MacBook Pro 16-inch M3 Max 128GB 8TB SSD",
            "Dell XPS 17 Intel i9 64GB 2TB OLED",
            "HP Spectre x360 16 Intel Evo i7 32GB",
            "Lenovo ThinkPad X1 Carbon Gen 11 32GB",
            "ASUS ZenBook Pro 16X OLED i9 64GB",
            "Microsoft Surface Laptop Studio 2 i7",
            "Razer Blade 18 RTX 4090 64GB 4TB",
            "Alienware x17 R2 RTX 4080 32GB"
        ],
        'accessories': [
            "Apple AirPods Pro 2nd Gen USB-C",
            "Sony WH-1000XM5 Wireless Headphones",
            "Bose QuietComfort 45 Bluetooth Headphones",
            "Logitech MX Master 3S Wireless Mouse",
            "Keychron K8 Pro QMK/VIA Wireless",
            "Apple Magic Keyboard with Touch ID",
            "Samsung T7 Portable SSD 4TB",
            "Anker PowerCore 26800 PD 45W"
        ]
    }
    
    # åœ°ç†ä½ç½®æ•°æ®é›†
    GEOGRAPHIC_LOCATIONS = {
        'cities_worldwide': [
            "New York, United States", "London, United Kingdom", "Tokyo, Japan",
            "Paris, France", "Sydney, Australia", "Toronto, Canada",
            "Berlin, Germany", "Amsterdam, Netherlands", "Stockholm, Sweden",
            "Copenhagen, Denmark", "Zurich, Switzerland", "Singapore, Singapore"
        ],
        'chinese_cities': [
            "åŒ—äº¬å¸‚ä¸œåŸåŒºç‹åºœäº•å¤§è¡—", "ä¸Šæµ·å¸‚é»„æµ¦åŒºå—äº¬ä¸œè·¯æ­¥è¡Œè¡—",
            "å¹¿å·å¸‚å¤©æ²³åŒºç æ±Ÿæ–°åŸCBD", "æ·±åœ³å¸‚å—å±±åŒºç§‘æŠ€å›­å—åŒº",
            "æ­å·å¸‚è¥¿æ¹–åŒºè¥¿æ¹–é£æ™¯åèƒœåŒº", "å—äº¬å¸‚ç„æ­¦åŒºç´«é‡‘å±±é£æ™¯åŒº",
            "æ­¦æ±‰å¸‚æ­¦æ˜ŒåŒºä¸œæ¹–é«˜æ–°æŠ€æœ¯å¼€å‘åŒº", "æˆéƒ½å¸‚é”¦æ±ŸåŒºæ˜¥ç†™è·¯å•†åœˆ",
            "è¥¿å®‰å¸‚é›å¡”åŒºé«˜æ–°æŠ€æœ¯äº§ä¸šå¼€å‘åŒº", "é‡åº†å¸‚æ¸ä¸­åŒºè§£æ”¾ç¢‘å•†åœˆ"
        ],
        'landmarks': [
            "Eiffel Tower, Paris, France", "Great Wall of China, Beijing",
            "Statue of Liberty, New York, USA", "Big Ben, London, UK",
            "Sydney Opera House, Sydney, Australia", "Taj Mahal, Agra, India",
            "Machu Picchu, Cusco Region, Peru", "Christ the Redeemer, Rio de Janeiro, Brazil",
            "Colosseum, Rome, Italy", "Petra, Ma'an Governorate, Jordan"
        ]
    }
    
    # æ–‡ä»¶ç³»ç»Ÿè·¯å¾„æ•°æ®é›†
    FILE_SYSTEM_PATHS = {
        'unix_paths': [
            "/home/user/documents/projects/2025/string_matcher/src/main.py",
            "/var/log/applications/web_server_access_20250130.log",
            "/usr/local/bin/python3.11",
            "/opt/software/database/config/production_settings.json",
            "/tmp/backup_files/database_dump_20250130_143025.sql",
            "/etc/nginx/sites-available/default_ssl.conf"
        ],
        'windows_paths': [
            "C:\\Users\\Administrator\\Desktop\\project_files\\StringMatcher.sln",
            "D:\\BackupData\\DatabaseBackups\\FullBackup_20250130_150000.bak",
            "E:\\Development\\Projects\\StringMatching\\Tests\\TestData.json",
            "F:\\Media\\Documents\\Reports\\Annual_Report_2024_Final.pdf",
            "G:\\Software\\IDEs\\VisualStudio2022\\Common7\\IDE\\devenv.exe",
            "H:\\Shared\\TeamDocuments\\Specifications\\API_Documentation_v2.3.docx"
        ],
        'network_paths': [
            "\\\\fileserver\\shared\\documents\\team_projects\\current\\",
            "\\\\backup-server\\daily-backups\\2025\\01\\30\\",
            "\\\\media-server\\video-library\\documentaries\\technology\\",
            "ftp://files.company.com/public/software/releases/v2.1.0/",
            "https://cdn.website.com/assets/images/products/high-res/",
            "sftp://secure-server.domain.com/encrypted/confidential/"
        ]
    }
    
    # ä¸“ä¸šæœ¯è¯­æ•°æ®é›†
    PROFESSIONAL_TERMS = {
        'software_engineering': [
            "Object-Oriented Programming (OOP)", "Test-Driven Development (TDD)",
            "Continuous Integration/Continuous Deployment (CI/CD)", 
            "Model-View-Controller (MVC) Architecture",
            "Application Programming Interface (API)",
            "Software Development Life Cycle (SDLC)",
            "Agile Software Development Methodology",
            "Version Control System (VCS)",
            "Database Management System (DBMS)",
            "Integrated Development Environment (IDE)"
        ],
        'data_science': [
            "Machine Learning Algorithm", "Deep Neural Network",
            "Natural Language Processing (NLP)", "Computer Vision",
            "Big Data Analytics", "Data Mining Techniques",
            "Statistical Analysis Methods", "Predictive Modeling",
            "Feature Engineering Process", "Model Performance Evaluation"
        ],
        'cybersecurity': [
            "Multi-Factor Authentication (MFA)", "Zero Trust Security Model",
            "Intrusion Detection System (IDS)", "Security Information and Event Management (SIEM)",
            "Penetration Testing Methodology", "Vulnerability Assessment",
            "Encryption Key Management", "Digital Certificate Authority",
            "Firewall Configuration Rules", "Incident Response Protocol"
        ]
    }
    
    # æ—¶é—´å’Œæ—¥æœŸæ ¼å¼æ•°æ®é›†
    DATETIME_FORMATS = {
        'date_formats': [
            "2025-01-30", "30/01/2025", "01-30-2025", "Jan 30, 2025",
            "January 30th, 2025", "30.01.2025", "2025å¹´1æœˆ30æ—¥", 
            "30-Jan-2025", "20250130", "30 January 2025"
        ],
        'time_formats': [
            "14:30:45", "2:30:45 PM", "14:30:45.123", "2:30 PM",
            "14:30", "02:30:45", "14h30m45s", "2:30:45.123 PM",
            "143045", "230PM"
        ],
        'datetime_combinations': [
            "2025-01-30T14:30:45Z", "2025-01-30 14:30:45 UTC",
            "Jan 30, 2025 at 2:30 PM EST", "30/01/2025 14:30:45",
            "2025å¹´1æœˆ30æ—¥ 14æ—¶30åˆ†45ç§’", "January 30, 2025, 2:30:45 PM",
            "Wed, 30 Jan 2025 14:30:45 GMT", "2025-01-30T14:30:45.123+08:00"
        ]
    }


class TestDataDrivenMatching(unittest.TestCase):
    """æ•°æ®é©±åŠ¨çš„åŒ¹é…æµ‹è¯•"""
    
    def setUp(self):
        self.exact_matcher = ExactStringMatcher()
        self.fuzzy_matcher = FuzzyStringMatcher(threshold=0.6)
        self.hybrid_matcher = HybridStringMatcher(fuzzy_threshold=0.6)
    
    def test_multilingual_name_matching(self):
        """æµ‹è¯•å¤šè¯­è¨€äººååŒ¹é…"""
        test_scenarios = [
            # è‹±æ–‡åæµ‹è¯•
            {
                'language': 'english',
                'candidates': TestDatasets.MULTILINGUAL_NAMES['english'],
                'test_cases': [
                    ("John Smith", "John Smith"),           # ç²¾ç¡®åŒ¹é…
                    ("john smith", "John Smith"),           # å¤§å°å†™ä¸åŒ
                    ("Jon Smith", "John Smith"),            # æ‹¼å†™é”™è¯¯
                    ("J. Smith", "John Smith"),             # ç¼©å†™å½¢å¼
                    ("Smith, John", "John Smith"),          # æ ¼å¼ä¸åŒ
                ]
            },
            # ä¸­æ–‡åæµ‹è¯•
            {
                'language': 'chinese', 
                'candidates': TestDatasets.MULTILINGUAL_NAMES['chinese'],
                'test_cases': [
                    ("å¼ ä¼Ÿ", "å¼ ä¼Ÿ"),                      # ç²¾ç¡®åŒ¹é…
                    ("å¼  ä¼Ÿ", "å¼ ä¼Ÿ"),                     # åŒ…å«ç©ºæ ¼
                    ("Zhang Wei", None),                   # æ‹¼éŸ³å½¢å¼ï¼ˆæœŸæœ›ä¸åŒ¹é…ï¼‰
                    ("ç‹èŠ³", "ç‹èŠ³"),                      # ç²¾ç¡®åŒ¹é…
                ]
            }
        ]
        
        for scenario in test_scenarios:
            language = scenario['language']
            candidates = scenario['candidates']
            
            for query, expected in scenario['test_cases']:
                with self.subTest(language=language, query=query):
                    # æµ‹è¯•ä¸åŒºåˆ†å¤§å°å†™çš„ç²¾ç¡®åŒ¹é…
                    case_insensitive_matcher = ExactStringMatcher(case_sensitive=False)
                    exact_result = case_insensitive_matcher.match_string(query, candidates)
                    
                    # æµ‹è¯•æ¨¡ç³ŠåŒ¹é…
                    fuzzy_result = self.fuzzy_matcher.match_string(query, candidates)
                    
                    # æµ‹è¯•æ··åˆåŒ¹é…
                    hybrid_result = self.hybrid_matcher.match_string(query, candidates)
                    
                    if expected is not None:
                        # è‡³å°‘æœ‰ä¸€ä¸ªåŒ¹é…å™¨åº”è¯¥æ‰¾åˆ°æ­£ç¡®ç»“æœ
                        results = [exact_result, fuzzy_result, hybrid_result]
                        self.assertIn(expected, results,
                                    f"None of the matchers found expected result '{expected}' for query '{query}'")
    
    def test_tech_product_matching(self):
        """æµ‹è¯•ç§‘æŠ€äº§å“åŒ¹é…"""
        all_products = []
        for category, products in TestDatasets.TECH_PRODUCTS.items():
            all_products.extend(products)
        
        test_cases = [
            # å“ç‰ŒåŒ¹é…
            ("iPhone", "iPhone 15 Pro Max 1TB Natural Titanium"),
            ("Samsung Galaxy", "Samsung Galaxy S24 Ultra 1TB Titanium Gray"),
            ("Google Pixel", "Google Pixel 8 Pro 512GB Obsidian"),
            ("MacBook", "MacBook Pro 16-inch M3 Max 128GB 8TB SSD"),
            
            # å‹å·åŒ¹é…
            ("15 Pro Max", "iPhone 15 Pro Max 1TB Natural Titanium"),
            ("S24 Ultra", "Samsung Galaxy S24 Ultra 1TB Titanium Gray"),
            ("XPS 17", "Dell XPS 17 Intel i9 64GB 2TB OLED"),
            
            # è§„æ ¼åŒ¹é…
            ("1TB", None),  # å¯èƒ½åŒ¹é…å¤šä¸ªäº§å“
            ("512GB", None),  # å¯èƒ½åŒ¹é…å¤šä¸ªäº§å“
            ("M3 Max", "MacBook Pro 16-inch M3 Max 128GB 8TB SSD"),
            
            # é¢œè‰²åŒ¹é…
            ("Natural Titanium", "iPhone 15 Pro Max 1TB Natural Titanium"),
            ("Titanium Gray", "Samsung Galaxy S24 Ultra 1TB Titanium Gray"),
            ("Obsidian", "Google Pixel 8 Pro 512GB Obsidian"),
        ]
        
        for query, expected in test_cases:
            with self.subTest(query=query):
                fuzzy_result = self.fuzzy_matcher.match_string(query, all_products)
                hybrid_result = self.hybrid_matcher.match_string(query, all_products)
                
                if expected is not None:
                    # è‡³å°‘æœ‰ä¸€ä¸ªåŒ¹é…å™¨åº”è¯¥æ‰¾åˆ°æœŸæœ›ç»“æœ
                    self.assertIn(expected, [fuzzy_result, hybrid_result],
                                f"No matcher found expected result '{expected}' for query '{query}'")
                
                # ç»“æœåº”è¯¥æ˜¯åˆç†çš„ï¼ˆéNoneæˆ–åœ¨å€™é€‰åˆ—è¡¨ä¸­ï¼‰
                if fuzzy_result is not None:
                    self.assertIn(fuzzy_result, all_products)
                if hybrid_result is not None:
                    self.assertIn(hybrid_result, all_products)
    
    def test_geographic_location_matching(self):
        """æµ‹è¯•åœ°ç†ä½ç½®åŒ¹é…"""
        all_locations = []
        for category, locations in TestDatasets.GEOGRAPHIC_LOCATIONS.items():
            all_locations.extend(locations)
        
        test_cases = [
            # åŸå¸‚ååŒ¹é…
            ("New York", "New York, United States"),
            ("London", "London, United Kingdom"),
            ("Tokyo", "Tokyo, Japan"),
            ("åŒ—äº¬", "åŒ—äº¬å¸‚ä¸œåŸåŒºç‹åºœäº•å¤§è¡—"),
            ("ä¸Šæµ·", "ä¸Šæµ·å¸‚é»„æµ¦åŒºå—äº¬ä¸œè·¯æ­¥è¡Œè¡—"),
            
            # åœ°æ ‡åŒ¹é…
            ("Eiffel Tower", "Eiffel Tower, Paris, France"),
            ("Great Wall", "Great Wall of China, Beijing"),
            ("Opera House", "Sydney Opera House, Sydney, Australia"),
            
            # åŒºåŸŸåŒ¹é…
            ("å¤©æ²³åŒº", "å¹¿å·å¸‚å¤©æ²³åŒºç æ±Ÿæ–°åŸCBD"),
            ("å—å±±åŒº", "æ·±åœ³å¸‚å—å±±åŒºç§‘æŠ€å›­å—åŒº"),
            ("é«˜æ–°åŒº", None),  # å¯èƒ½åŒ¹é…å¤šä¸ªåœ°ç‚¹
        ]
        
        for query, expected in test_cases:
            with self.subTest(query=query):
                fuzzy_result = self.fuzzy_matcher.match_string(query, all_locations)
                hybrid_result = self.hybrid_matcher.match_string(query, all_locations)
                
                if expected is not None:
                    # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°æœŸæœ›ç»“æœ
                    self.assertIn(expected, [fuzzy_result, hybrid_result],
                                f"No matcher found expected result '{expected}' for query '{query}'")
    
    def test_file_path_matching(self):
        """æµ‹è¯•æ–‡ä»¶è·¯å¾„åŒ¹é…"""
        all_paths = []
        for category, paths in TestDatasets.FILE_SYSTEM_PATHS.items():
            all_paths.extend(paths)
        
        test_cases = [
            # æ–‡ä»¶ååŒ¹é…
            ("main.py", "/home/user/documents/projects/2025/string_matcher/src/main.py"),
            ("StringMatcher.sln", "C:\\Users\\Administrator\\Desktop\\project_files\\StringMatcher.sln"),
            ("TestData.json", "E:\\Development\\Projects\\StringMatching\\Tests\\TestData.json"),
            
            # ç›®å½•åŒ¹é…
            ("documents", None),  # å¯èƒ½åŒ¹é…å¤šä¸ªè·¯å¾„
            ("Desktop", "C:\\Users\\Administrator\\Desktop\\project_files\\StringMatcher.sln"),
            ("projects", None),   # å¯èƒ½åŒ¹é…å¤šä¸ªè·¯å¾„
            
            # æ‰©å±•ååŒ¹é…
            (".py", "/home/user/documents/projects/2025/string_matcher/src/main.py"),
            (".log", "/var/log/applications/web_server_access_20250130.log"),
            (".sql", "/tmp/backup_files/database_dump_20250130_143025.sql"),
            
            # ç‰¹æ®Šè·¯å¾„åŒ¹é…
            ("fileserver", "\\\\fileserver\\shared\\documents\\team_projects\\current\\"),
            ("https://", "https://cdn.website.com/assets/images/products/high-res/"),
        ]
        
        for query, expected in test_cases:
            with self.subTest(query=query):
                fuzzy_result = self.fuzzy_matcher.match_string(query, all_paths)
                hybrid_result = self.hybrid_matcher.match_string(query, all_paths)
                
                if expected is not None:
                    self.assertIn(expected, [fuzzy_result, hybrid_result],
                                f"No matcher found expected result '{expected}' for query '{query}'")
    
    def test_professional_terms_matching(self):
        """æµ‹è¯•ä¸“ä¸šæœ¯è¯­åŒ¹é…"""
        all_terms = []
        for category, terms in TestDatasets.PROFESSIONAL_TERMS.items():
            all_terms.extend(terms)
        
        test_cases = [
            # ç¼©å†™åŒ¹é…
            ("OOP", "Object-Oriented Programming (OOP)"),
            ("TDD", "Test-Driven Development (TDD)"),
            ("CI/CD", "Continuous Integration/Continuous Deployment (CI/CD)"),
            ("MVC", "Model-View-Controller (MVC) Architecture"),
            ("API", "Application Programming Interface (API)"),
            
            # éƒ¨åˆ†æœ¯è¯­åŒ¹é…
            ("Machine Learning", "Machine Learning Algorithm"),
            ("Neural Network", "Deep Neural Network"),
            ("Natural Language", "Natural Language Processing (NLP)"),
            ("Multi-Factor", "Multi-Factor Authentication (MFA)"),
            
            # ç›¸ä¼¼æœ¯è¯­åŒ¹é…
            ("Deep Learning", "Deep Neural Network"),
            ("Computer Vision", "Computer Vision"),
            ("Penetration Test", "Penetration Testing Methodology"),
        ]
        
        for query, expected in test_cases:
            with self.subTest(query=query):
                fuzzy_result = self.fuzzy_matcher.match_string(query, all_terms)
                hybrid_result = self.hybrid_matcher.match_string(query, all_terms)
                
                if expected is not None:
                    self.assertIn(expected, [fuzzy_result, hybrid_result],
                                f"No matcher found expected result '{expected}' for query '{query}'")
    
    def test_datetime_format_matching(self):
        """æµ‹è¯•æ—¥æœŸæ—¶é—´æ ¼å¼åŒ¹é…"""
        all_formats = []
        for category, formats in TestDatasets.DATETIME_FORMATS.items():
            all_formats.extend(formats)
        
        test_cases = [
            # æ ‡å‡†æ ¼å¼åŒ¹é…
            ("2025-01-30", "2025-01-30"),
            ("30/01/2025", "30/01/2025"),
            ("14:30:45", "14:30:45"),
            
            # æ ¼å¼å˜ä½“åŒ¹é…
            ("2025/01/30", "30/01/2025"),    # ç›¸ä¼¼æ ¼å¼
            ("Jan 30", "Jan 30, 2025"),      # éƒ¨åˆ†åŒ¹é…
            ("14:30", "14:30:45"),           # éƒ¨åˆ†åŒ¹é…
            
            # æœ¬åœ°åŒ–æ ¼å¼åŒ¹é…
            ("2025å¹´1æœˆ30æ—¥", "2025å¹´1æœˆ30æ—¥"),
            ("January 30", "January 30th, 2025"),
        ]
        
        for query, expected in test_cases:
            with self.subTest(query=query):
                fuzzy_result = self.fuzzy_matcher.match_string(query, all_formats)
                hybrid_result = self.hybrid_matcher.match_string(query, all_formats)
                
                if expected is not None:
                    self.assertIn(expected, [fuzzy_result, hybrid_result],
                                f"No matcher found expected result '{expected}' for query '{query}'")


class TestSimilarityValidation(unittest.TestCase):
    """ç›¸ä¼¼åº¦è®¡ç®—éªŒè¯æµ‹è¯•"""
    
    def test_similarity_consistency_across_datasets(self):
        """æµ‹è¯•ä¸åŒæ•°æ®é›†ä¸­ç›¸ä¼¼åº¦è®¡ç®—çš„ä¸€è‡´æ€§"""
        test_datasets = [
            TestDatasets.MULTILINGUAL_NAMES['english'],
            TestDatasets.TECH_PRODUCTS['smartphones'], 
            TestDatasets.GEOGRAPHIC_LOCATIONS['cities_worldwide'],
            TestDatasets.PROFESSIONAL_TERMS['software_engineering']
        ]
        
        for dataset in test_datasets:
            dataset_name = str(dataset[:2])  # ä½¿ç”¨å‰ä¸¤ä¸ªå…ƒç´ ä½œä¸ºæ ‡è¯†
            
            with self.subTest(dataset=dataset_name):
                # æµ‹è¯•è‡ªç›¸ä¼¼æ€§
                for item in dataset[:3]:  # æµ‹è¯•å‰3ä¸ªé¡¹ç›®
                    similarity = SimilarityCalculator.calculate_similarity(item, item)
                    self.assertEqual(similarity, 1.0, f"Self-similarity should be 1.0 for '{item}'")
                
                # æµ‹è¯•å¯¹ç§°æ€§
                if len(dataset) >= 2:
                    item1, item2 = dataset[0], dataset[1]
                    sim1 = SimilarityCalculator.calculate_similarity(item1, item2)
                    sim2 = SimilarityCalculator.calculate_similarity(item2, item1)
                    self.assertEqual(sim1, sim2, f"Similarity should be symmetric for '{item1}' and '{item2}'")
                
                # æµ‹è¯•ä¸‰è§’ä¸ç­‰å¼ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                if len(dataset) >= 3:
                    item1, item2, item3 = dataset[0], dataset[1], dataset[2]
                    sim12 = SimilarityCalculator.calculate_similarity(item1, item2)
                    sim23 = SimilarityCalculator.calculate_similarity(item2, item3)
                    sim13 = SimilarityCalculator.calculate_similarity(item1, item3)
                    
                    # ç›¸ä¼¼åº¦çš„"è·ç¦»"åº”è¯¥æ»¡è¶³ä¸‰è§’ä¸ç­‰å¼ï¼šd(a,c) <= d(a,b) + d(b,c)
                    # å…¶ä¸­ d(x,y) = 1 - similarity(x,y)
                    dist12 = 1 - sim12
                    dist23 = 1 - sim23 
                    dist13 = 1 - sim13
                    
                    self.assertLessEqual(dist13, dist12 + dist23 + 0.1,  # å…è®¸å°çš„æ•°å€¼è¯¯å·®
                                       f"Triangle inequality violated for '{item1}', '{item2}', '{item3}'")


class TestMatcherRobustness(unittest.TestCase):
    """åŒ¹é…å™¨å¥å£®æ€§æµ‹è¯•"""
    
    def test_special_characters_handling(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        special_char_data = [
            "file@2025-01-30.txt",
            "user_name_with_underscores",
            "path/with/forward/slashes",
            "path\\with\\backward\\slashes",
            "string with spaces",
            "string-with-hyphens",
            "string.with.dots",
            "string,with,commas",
            "string;with;semicolons",
            "string:with:colons",
            "string(with)parentheses",
            "string[with]brackets",
            "string{with}braces",
            "string<with>angle_brackets",
            "string'with'quotes",
            "string\"with\"double_quotes",
            "string+with+plus",
            "string=with=equals",
            "string&with&ampersand",
            "string%with%percent",
            "string#with#hash",
            "string!with!exclamation",
            "string?with?question",
        ]
        
        matchers = [
            self.exact_matcher,
            self.fuzzy_matcher, 
            self.hybrid_matcher
        ]
        
        for matcher in matchers:
            matcher_name = matcher.__class__.__name__
            
            for special_string in special_char_data:
                with self.subTest(matcher=matcher_name, string=special_string):
                    # æµ‹è¯•è‡ªåŒ¹é…
                    result = matcher.match_string(special_string, special_char_data)
                    
                    if isinstance(matcher, ExactStringMatcher):
                        self.assertEqual(result, special_string)
                    else:
                        # æ¨¡ç³ŠåŒ¹é…å’Œæ··åˆåŒ¹é…è‡³å°‘åº”è¯¥æ‰¾åˆ°è‡ªå·±
                        self.assertIsNotNone(result)
    
    def test_unicode_handling(self):
        """æµ‹è¯•Unicodeå­—ç¬¦å¤„ç†"""
        unicode_data = [
            "Hello ä¸–ç•Œ",                    # è‹±æ–‡+ä¸­æ–‡
            "CafÃ© â˜• Coffee",                # ç‰¹æ®Šå­—ç¬¦+Emoji
            "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€",                   # ä¿„æ–‡
            "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…",                # é˜¿æ‹‰ä¼¯æ–‡
            "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ",                # æ—¥æ–‡
            "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„",               # éŸ©æ–‡
            "ğŸŒŸ Star â­ Light",            # Emoji
            "Math: Ï€ â‰ˆ 3.14159",           # æ•°å­¦ç¬¦å·
            "Currency: â‚¬ $ Â¥ Â£",           # è´§å¸ç¬¦å·
            "Arrows: â† â†’ â†‘ â†“",             # ç®­å¤´ç¬¦å·
        ]
        
        matchers = [
            ExactStringMatcher(case_sensitive=False),
            FuzzyStringMatcher(threshold=0.5),
            HybridStringMatcher(fuzzy_threshold=0.5)
        ]
        
        for matcher in matchers:
            matcher_name = matcher.__class__.__name__
            
            for unicode_string in unicode_data:
                with self.subTest(matcher=matcher_name, string=unicode_string):
                    try:
                        # æµ‹è¯•Unicodeå­—ç¬¦ä¸²çš„è‡ªåŒ¹é…
                        result = matcher.match_string(unicode_string, unicode_data)
                        
                        # åº”è¯¥è‡³å°‘æ‰¾åˆ°è‡ªå·±æˆ–ç›¸ä¼¼çš„å­—ç¬¦ä¸²
                        self.assertIsNotNone(result, f"Failed to match Unicode string: {unicode_string}")
                        
                    except Exception as e:
                        self.fail(f"{matcher_name} failed with Unicode string '{unicode_string}': {e}")
    
    def test_extreme_length_strings(self):
        """æµ‹è¯•æç«¯é•¿åº¦å­—ç¬¦ä¸²"""
        # å¾ˆçŸ­çš„å­—ç¬¦ä¸²
        short_strings = ["a", "ab", "abc", "ä¸­", "ğŸŒŸ"]
        
        # å¾ˆé•¿çš„å­—ç¬¦ä¸²
        long_strings = [
            "a" * 1000,
            "This is a very long string that contains many words and should test the performance and correctness of string matching algorithms when dealing with lengthy text inputs that might occur in real-world applications." * 10,
            "ä¸­æ–‡å¾ˆé•¿çš„å­—ç¬¦ä¸²æµ‹è¯•" * 100,
        ]
        
        test_cases = [
            ("Short strings", short_strings),
            ("Long strings", long_strings)
        ]
        
        matchers = [
            ExactStringMatcher(),
            FuzzyStringMatcher(threshold=0.8),  # ä½¿ç”¨è¾ƒé«˜é˜ˆå€¼æé«˜æ€§èƒ½
        ]
        
        for test_name, test_strings in test_cases:
            for matcher in matchers:
                matcher_name = matcher.__class__.__name__
                
                with self.subTest(test=test_name, matcher=matcher_name):
                    for test_string in test_strings:
                        try:
                            # æµ‹è¯•è‡ªåŒ¹é…
                            result = matcher.match_string(test_string, test_strings)
                            
                            if isinstance(matcher, ExactStringMatcher):
                                self.assertEqual(result, test_string)
                            else:
                                # æ¨¡ç³ŠåŒ¹é…åº”è¯¥è‡³å°‘æ‰¾åˆ°è‡ªå·±æˆ–ç›¸ä¼¼å­—ç¬¦ä¸²
                                self.assertIsNotNone(result)
                                
                        except Exception as e:
                            self.fail(f"{matcher_name} failed with {test_name.lower()} string: {e}")


if __name__ == '__main__':
    print("Running Data-Driven String Matcher Tests...")
    print("=" * 60)
    
    unittest.main(verbosity=2)
