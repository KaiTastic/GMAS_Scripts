#!/usr/bin/env python3
"""
KMZæ–‡ä»¶æ•°æ®é›†åˆ†æå™¨
åˆ†ææ”¶é›†åˆ°çš„KMZæ–‡ä»¶æ•°æ®é›†ï¼Œä¸ºå­—ç¬¦ä¸²åŒ¹é…æä¾›åŸºç¡€æ•°æ®
"""

import pandas as pd
import re
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime
import json

class KMZDatasetAnalyzer:
    def __init__(self, csv_file_path: str):
        """
        åˆå§‹åŒ–KMZæ•°æ®é›†åˆ†æå™¨
        
        Args:
            csv_file_path: CSVæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        """
        self.csv_file_path = csv_file_path
        self.df = None
        
    def load_data(self):
        """åŠ è½½CSVæ•°æ®"""
        try:
            self.df = pd.read_csv(self.csv_file_path, encoding='utf-8')
            print(f"æˆåŠŸåŠ è½½ {len(self.df)} ä¸ªKMZæ–‡ä»¶è®°å½•")
            return True
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def extract_date_patterns(self, filename: str):
        """
        ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸæ¨¡å¼
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            dict: åŒ…å«å„ç§æ—¥æœŸä¿¡æ¯çš„å­—å…¸
        """
        result = {
            'date_8digit': None,  # YYYYMMDD
            'date_6digit': None,  # YYMMDD
            'year': None,
            'month': None,
            'day': None
        }
        
        # 8ä½æ—¥æœŸ (YYYYMMDD)
        pattern_8 = r'(\d{8})'
        match_8 = re.search(pattern_8, filename)
        if match_8:
            date_str = match_8.group(1)
            try:
                date_obj = datetime.strptime(date_str, '%Y%m%d')
                result['date_8digit'] = date_str
                result['year'] = date_obj.year
                result['month'] = date_obj.month
                result['day'] = date_obj.day
            except ValueError:
                pass
        
        # 6ä½æ—¥æœŸ (YYMMDD)
        if not result['date_8digit']:
            pattern_6 = r'(\d{6})'
            match_6 = re.search(pattern_6, filename)
            if match_6:
                date_str = match_6.group(1)
                try:
                    date_obj = datetime.strptime(date_str, '%y%m%d')
                    result['date_6digit'] = date_str
                    result['year'] = date_obj.year
                    result['month'] = date_obj.month
                    result['day'] = date_obj.day
                except ValueError:
                    pass
        
        return result
    
    def extract_name_patterns(self, filename: str):
        """
        ä»æ–‡ä»¶åä¸­æå–åœ°åå’Œæ¨¡å¼
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            dict: åŒ…å«åœ°åå’Œæ¨¡å¼ä¿¡æ¯çš„å­—å…¸
        """
        # ç§»é™¤æ‰©å±•å
        name_without_ext = filename.replace('.kmz', '').replace('.KMZ', '')
        
        result = {
            'base_name': name_without_ext,
            'has_finished_points_and_tracks': ('finished_points_and_tracks' in name_without_ext.lower() or 
                                              'finished_points' in name_without_ext.lower() or 
                                              'finished' in name_without_ext.lower()),
            'has_plan_routes': 'plan_routes' in name_without_ext.lower() or 'plan_route' in name_without_ext.lower(),
            'location_name': None,
            'pattern_type': None
        }
        
        # æå–ä½ç½®åç§°ï¼ˆå‡è®¾æ˜¯ä¸‹åˆ’çº¿åˆ†éš”çš„ç¬¬ä¸€éƒ¨åˆ†ï¼‰
        parts = name_without_ext.split('_')
        if len(parts) > 0:
            result['location_name'] = parts[0]
        
        # ç¡®å®šæ¨¡å¼ç±»å‹
        if result['has_finished_points_and_tracks']:
            result['pattern_type'] = 'finished_points_and_tracks'
        elif result['has_plan_routes']:
            result['pattern_type'] = 'plan_routes'
        else:
            result['pattern_type'] = 'other'
        
        return result
    
    def exact_pattern_match(self, filename: str):
        """
        ç²¾ç¡®åŒ¹é…æ–‡ä»¶åæ¨¡å¼
        ä¸¥æ ¼æŒ‰ç…§é¢„æœŸæ ¼å¼è¿›è¡ŒåŒ¹é…ï¼šä½ç½®å_finished_points_and_tracks_YYYYMMDD
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            dict: ç²¾ç¡®åŒ¹é…ç»“æœ
        """
        # ç§»é™¤æ‰©å±•å
        name_without_ext = filename.replace('.kmz', '').replace('.KMZ', '')
        
        result = {
            'is_exact_match': False,
            'location_part': None,
            'middle_part': None,
            'date_part': None,
            'exact_pattern_type': None
        }
        
        # ç²¾ç¡®åŒ¹é…æ¨¡å¼1: ä½ç½®å_finished_points_and_tracks_YYYYMMDD
        pattern1 = r'^(.+?)_(finished_points_and_tracks)_(\d{8})(?:\(\d+\))?$'
        match1 = re.match(pattern1, name_without_ext, re.IGNORECASE)
        
        if match1:
            result['is_exact_match'] = True
            result['location_part'] = match1.group(1)
            result['middle_part'] = match1.group(2)
            result['date_part'] = match1.group(3)
            result['exact_pattern_type'] = 'finished_points_and_tracks'
            return result
        
        # ç²¾ç¡®åŒ¹é…æ¨¡å¼2: ä½ç½®å_plan_routes_YYYYMMDD
        pattern2 = r'^(.+?)_(plan_routes?)_(\d{8})(?:\(\d+\))?$'
        match2 = re.match(pattern2, name_without_ext, re.IGNORECASE)
        
        if match2:
            result['is_exact_match'] = True
            result['location_part'] = match2.group(1)
            result['middle_part'] = match2.group(2)
            result['date_part'] = match2.group(3)
            result['exact_pattern_type'] = 'plan_routes'
            return result
        
        # ç²¾ç¡®åŒ¹é…æ¨¡å¼3: GMAS_Points_and_tracks_until_YYYYMMDD
        pattern3 = r'^(GMAS)_(Points_and_tracks_until)_(\d{8})(?:\(\d+\))?$'
        match3 = re.match(pattern3, name_without_ext, re.IGNORECASE)
        
        if match3:
            result['is_exact_match'] = True
            result['location_part'] = match3.group(1)
            result['middle_part'] = match3.group(2)
            result['date_part'] = match3.group(3)
            result['exact_pattern_type'] = 'gmas_points'
            return result
        
        return result
    
    def fuzzy_location_match(self, location_candidate: str, known_locations: list):
        """
        å¯¹ä½ç½®åç§°è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
        
        Args:
            location_candidate: å€™é€‰ä½ç½®åç§°
            known_locations: å·²çŸ¥ä½ç½®åç§°åˆ—è¡¨
            
        Returns:
            dict: ä½ç½®åŒ¹é…ç»“æœ
        """
        if not location_candidate:
            return {'match': None, 'confidence': 0.0, 'method': 'none'}
        
        location_candidate = location_candidate.strip()
        best_match = None
        best_confidence = 0.0
        best_method = 'none'
        
        for known_loc in known_locations:
            if not known_loc:
                continue
                
            # 1. å®Œå…¨åŒ¹é…
            if location_candidate.lower() == known_loc.lower():
                return {'match': known_loc, 'confidence': 1.0, 'method': 'exact'}
            
            # 2. åŒ…å«åŒ¹é…
            if location_candidate.lower() in known_loc.lower():
                confidence = len(location_candidate) / len(known_loc)
                if confidence > best_confidence:
                    best_match = known_loc
                    best_confidence = confidence
                    best_method = 'contains'
            
            if known_loc.lower() in location_candidate.lower():
                confidence = len(known_loc) / len(location_candidate)
                if confidence > best_confidence:
                    best_match = known_loc
                    best_confidence = confidence
                    best_method = 'contained'
            
            # 3. å‰ç¼€åŒ¹é…
            if location_candidate.lower().startswith(known_loc.lower()[:3]) and len(known_loc) >= 3:
                confidence = 0.7
                if confidence > best_confidence:
                    best_match = known_loc
                    best_confidence = confidence
                    best_method = 'prefix'
            
            # 4. å»é™¤ç‰¹æ®Šå­—ç¬¦ååŒ¹é…
            clean_candidate = re.sub(r'[^a-zA-Z]', '', location_candidate).lower()
            clean_known = re.sub(r'[^a-zA-Z]', '', known_loc).lower()
            
            if clean_candidate == clean_known and len(clean_candidate) > 2:
                confidence = 0.9
                if confidence > best_confidence:
                    best_match = known_loc
                    best_confidence = confidence
                    best_method = 'clean_match'
        
        return {'match': best_match, 'confidence': best_confidence, 'method': best_method}
    
    def fuzzy_middle_part_match(self, middle_candidate: str):
        """
        å¯¹ä¸­é—´éƒ¨åˆ†ï¼ˆå…³é”®è¯ï¼‰è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
        
        Args:
            middle_candidate: å€™é€‰ä¸­é—´éƒ¨åˆ†
            
        Returns:
            dict: ä¸­é—´éƒ¨åˆ†åŒ¹é…ç»“æœ
        """
        if not middle_candidate:
            return {'pattern_type': None, 'confidence': 0.0, 'keywords_found': [], 'issues': []}
        
        middle_lower = middle_candidate.lower()
        
        # å®šä¹‰æ¨¡å¼å…³é”®è¯å’Œå˜ä½“
        pattern_keywords = {
            'finished_points_and_tracks': {
                'required': ['finished', 'points'],
                'optional': ['tracks', 'and'],
                'variants': {
                    'finished': ['finish', 'complete', 'done'],
                    'points': ['point', 'pts', 'pt'],
                    'tracks': ['track', 'trk', 'route', 'path'],
                    'and': ['&', 'n']
                }
            },
            'plan_routes': {
                'required': ['plan', 'route'],
                'optional': [],
                'variants': {
                    'plan': ['planning', 'planned', 'planed'],
                    'route': ['routes', 'routing', 'path', 'track']
                }
            },
            'gmas_points': {
                'required': ['gmas'],
                'optional': ['points'],
                'variants': {
                    'gmas': ['gms'],
                    'points': ['point', 'pts', 'pt']
                }
            }
        }
        
        results = {}
        
        for pattern_type, config in pattern_keywords.items():
            score = 0
            found_keywords = []
            confidence = 0.0
            issues = []
            
            # æ£€æŸ¥å¿…éœ€å…³é”®è¯
            required_found = 0
            for req_keyword in config['required']:
                if req_keyword in middle_lower:
                    score += 2
                    required_found += 1
                    found_keywords.append(req_keyword)
                else:
                    # æ£€æŸ¥å˜ä½“
                    variants = config['variants'].get(req_keyword, [])
                    for variant in variants:
                        if variant in middle_lower:
                            score += 1.5
                            required_found += 1
                            found_keywords.append(f"{req_keyword}({variant})")
                            break
            
            # æ£€æŸ¥å¯é€‰å…³é”®è¯
            for opt_keyword in config['optional']:
                if opt_keyword in middle_lower:
                    score += 1
                    found_keywords.append(opt_keyword)
                else:
                    # æ£€æŸ¥å˜ä½“
                    variants = config['variants'].get(opt_keyword, [])
                    for variant in variants:
                        if variant in middle_lower:
                            score += 0.8
                            found_keywords.append(f"{opt_keyword}({variant})")
                            break
            
            # è®¡ç®—ç½®ä¿¡åº¦
            total_required = len(config['required'])
            if total_required > 0:
                confidence = (required_found / total_required) * 0.8 + (score / (total_required * 2 + len(config['optional']))) * 0.2
            
            # æ£€æŸ¥é—®é¢˜
            if required_found < total_required:
                issues.append(f"ç¼ºå°‘å¿…éœ€å…³é”®è¯: {[kw for kw in config['required'] if kw not in middle_lower]}")
            
            if confidence > 0:
                results[pattern_type] = {
                    'confidence': confidence,
                    'keywords_found': found_keywords,
                    'score': score,
                    'issues': issues
                }
        
        if not results:
            return {'pattern_type': None, 'confidence': 0.0, 'keywords_found': [], 'issues': ['æ— æ³•è¯†åˆ«ä»»ä½•å·²çŸ¥æ¨¡å¼']}
        
        # é€‰æ‹©æœ€ä½³åŒ¹é…
        best_pattern = max(results.items(), key=lambda x: x[1]['confidence'])
        return {
            'pattern_type': best_pattern[0],
            'confidence': best_pattern[1]['confidence'],
            'keywords_found': best_pattern[1]['keywords_found'],
            'issues': best_pattern[1]['issues']
        }
    
    def fuzzy_date_match(self, date_candidate: str):
        """
        å¯¹æ—¥æœŸéƒ¨åˆ†è¿›è¡Œæ¨¡ç³ŠåŒ¹é…å’Œä¿®å¤
        
        Args:
            date_candidate: å€™é€‰æ—¥æœŸå­—ç¬¦ä¸²
            
        Returns:
            dict: æ—¥æœŸåŒ¹é…ç»“æœ
        """
        if not date_candidate:
            return {'date': None, 'confidence': 0.0, 'format': None, 'issues': []}
        
        issues = []
        
        # æå–æ‰€æœ‰å¯èƒ½çš„æ•°å­—åºåˆ—
        number_sequences = re.findall(r'\d+', date_candidate)
        
        for seq in number_sequences:
            # 1. æ ‡å‡†8ä½æ—¥æœŸæ ¼å¼ YYYYMMDD
            if len(seq) == 8:
                try:
                    datetime.strptime(seq, '%Y%m%d')
                    return {'date': seq, 'confidence': 1.0, 'format': 'YYYYMMDD', 'issues': []}
                except ValueError:
                    issues.append(f"8ä½æ•°å­—{seq}ä¸æ˜¯æœ‰æ•ˆæ—¥æœŸ")
            
            # 2. 6ä½æ—¥æœŸæ ¼å¼ YYMMDD
            elif len(seq) == 6:
                try:
                    # å°è¯•ä½œä¸ºYYMMDDè§£æ
                    parsed_date = datetime.strptime(seq, '%y%m%d')
                    full_date = parsed_date.strftime('%Y%m%d')
                    return {'date': full_date, 'confidence': 0.9, 'format': 'YYMMDD->YYYYMMDD', 'issues': []}
                except ValueError:
                    issues.append(f"6ä½æ•°å­—{seq}ä¸æ˜¯æœ‰æ•ˆYYMMDDæ ¼å¼")
            
            # 3. å¤„ç†å¯èƒ½çš„æ—¥æœŸå˜ä½“
            elif len(seq) > 8:
                # å¯èƒ½æ˜¯æ—¥æœŸ+é¢å¤–æ•°å­—
                for i in range(len(seq) - 7):
                    substr = seq[i:i+8]
                    try:
                        datetime.strptime(substr, '%Y%m%d')
                        return {
                            'date': substr, 
                            'confidence': 0.8, 
                            'format': f'extracted_from_{seq}',
                            'issues': [f"ä»{seq}ä¸­æå–äº†{substr}"]
                        }
                    except ValueError:
                        continue
                
                # å°è¯•å‰8ä½
                if len(seq) >= 8:
                    front_8 = seq[:8]
                    try:
                        datetime.strptime(front_8, '%Y%m%d')
                        return {
                            'date': front_8,
                            'confidence': 0.7,
                            'format': f'front_8_from_{seq}',
                            'issues': [f"ä½¿ç”¨äº†{seq}çš„å‰8ä½: {front_8}"]
                        }
                    except ValueError:
                        issues.append(f"å‰8ä½{front_8}ä¸æ˜¯æœ‰æ•ˆæ—¥æœŸ")
                
                # å°è¯•å8ä½
                if len(seq) >= 8:
                    back_8 = seq[-8:]
                    try:
                        datetime.strptime(back_8, '%Y%m%d')
                        return {
                            'date': back_8,
                            'confidence': 0.6,
                            'format': f'back_8_from_{seq}',
                            'issues': [f"ä½¿ç”¨äº†{seq}çš„å8ä½: {back_8}"]
                        }
                    except ValueError:
                        issues.append(f"å8ä½{back_8}ä¸æ˜¯æœ‰æ•ˆæ—¥æœŸ")
        
        # 4. å°è¯•ç»„åˆå¤šä¸ªæ•°å­—åºåˆ—
        if len(number_sequences) >= 3:
            # å¯èƒ½æ˜¯å¹´-æœˆ-æ—¥åˆ†å¼€çš„æ ¼å¼
            try:
                if len(number_sequences[0]) == 4:  # YYYY
                    year = number_sequences[0]
                    month = number_sequences[1].zfill(2)
                    day = number_sequences[2].zfill(2)
                    combined_date = year + month + day
                    datetime.strptime(combined_date, '%Y%m%d')
                    return {
                        'date': combined_date,
                        'confidence': 0.8,
                        'format': 'combined_YYYY_MM_DD',
                        'issues': [f"ä»åˆ†ç¦»çš„æ•°å­—{number_sequences[:3]}ç»„åˆè€Œæˆ"]
                    }
            except (ValueError, IndexError):
                issues.append("æ— æ³•ä»åˆ†ç¦»çš„æ•°å­—ç»„åˆæˆæœ‰æ•ˆæ—¥æœŸ")
        
        return {
            'date': None,
            'confidence': 0.0,
            'format': None,
            'issues': issues + [f"æ— æ³•ä»'{date_candidate}'ä¸­æå–æœ‰æ•ˆæ—¥æœŸ"]
        }
    
    def fuzzy_pattern_match(self, filename: str):
        """
        å¯¹æ— æ³•ç²¾ç¡®åŒ¹é…çš„æ–‡ä»¶è¿›è¡Œå¢å¼ºçš„æ¨¡ç³ŠåŒ¹é…
        ä½¿ç”¨åˆ†ç»„ä»¶çš„ä¸“é—¨åŒ¹é…é€»è¾‘
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            dict: å¢å¼ºçš„æ¨¡ç³ŠåŒ¹é…ç»“æœ
        """
        # ç§»é™¤æ‰©å±•å
        name_without_ext = filename.replace('.kmz', '').replace('.KMZ', '')
        
        result = {
            'is_fuzzy_match': False,
            'location_part': None,
            'middle_part': None,
            'date_part': None,
            'fuzzy_pattern_type': None,
            'confidence': 0.0,
            'issues': [],
            'component_analysis': {}
        }
        
        # åˆ†å‰²æ–‡ä»¶åä¸ºå¯èƒ½çš„ç»„ä»¶
        parts = name_without_ext.split('_')
        
        # 1. æ—¥æœŸç»„ä»¶åˆ†æ
        date_analysis = self.fuzzy_date_match(name_without_ext)
        result['component_analysis']['date'] = date_analysis
        
        if date_analysis['date']:
            result['date_part'] = date_analysis['date']
            result['confidence'] += date_analysis['confidence'] * 0.4
            if date_analysis['issues']:
                result['issues'].extend([f"æ—¥æœŸ: {issue}" for issue in date_analysis['issues']])
        else:
            result['issues'].append("æ— æ³•è¯†åˆ«æœ‰æ•ˆæ—¥æœŸ")
        
        # 2. ä¸­é—´éƒ¨åˆ†ï¼ˆå…³é”®è¯ï¼‰åˆ†æ
        middle_analysis = self.fuzzy_middle_part_match(name_without_ext)
        result['component_analysis']['middle'] = middle_analysis
        
        if middle_analysis['pattern_type']:
            result['fuzzy_pattern_type'] = middle_analysis['pattern_type']
            result['middle_part'] = middle_analysis['pattern_type']
            result['confidence'] += middle_analysis['confidence'] * 0.4
            if middle_analysis['issues']:
                result['issues'].extend([f"å…³é”®è¯: {issue}" for issue in middle_analysis['issues']])
        else:
            result['issues'].append("æ— æ³•è¯†åˆ«å·²çŸ¥çš„æ–‡ä»¶ç±»å‹æ¨¡å¼")
        
        # 3. ä½ç½®ç»„ä»¶åˆ†æ
        # è·å–å·²çŸ¥ä½ç½®åˆ—è¡¨ï¼ˆä»æ•°æ®é›†ä¸­æå–ï¼‰
        known_locations = self.get_known_locations()
        
        # å°è¯•ä»æ–‡ä»¶åä¸­æå–å€™é€‰ä½ç½®
        location_candidates = self.extract_location_candidates(name_without_ext, result['date_part'], result['middle_part'])
        
        best_location_match = None
        best_location_confidence = 0.0
        
        for candidate in location_candidates:
            location_analysis = self.fuzzy_location_match(candidate, known_locations)
            result['component_analysis'][f'location_{candidate}'] = location_analysis
            
            if location_analysis['confidence'] > best_location_confidence:
                best_location_match = location_analysis
                best_location_confidence = location_analysis['confidence']
        
        if best_location_match and best_location_match['match']:
            result['location_part'] = best_location_match['match']
            result['confidence'] += best_location_confidence * 0.2
            if best_location_match['method'] != 'exact':
                result['issues'].append(f"ä½ç½®ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…: {best_location_match['method']}")
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„å·²çŸ¥ä½ç½®ï¼Œä½¿ç”¨æœ€å¯èƒ½çš„å€™é€‰ä½ç½®
            if location_candidates:
                result['location_part'] = location_candidates[0]
                result['confidence'] += 0.1
                result['issues'].append("ä½ç½®ä¸ºæœªçŸ¥åœ°åï¼Œä½¿ç”¨æå–çš„å€™é€‰åç§°")
            else:
                result['issues'].append("æ— æ³•æå–ä½ç½®ä¿¡æ¯")
        
        # 4. æœ€ç»ˆè¯„ä¼°
        # åªæœ‰å½“æˆ‘ä»¬è‡³å°‘æœ‰æ—¥æœŸæˆ–æ¨¡å¼ç±»å‹æ—¶æ‰è®¤ä¸ºæ˜¯æˆåŠŸçš„æ¨¡ç³ŠåŒ¹é…
        if result['date_part'] or result['fuzzy_pattern_type']:
            result['is_fuzzy_match'] = True
            
            # è°ƒæ•´ç½®ä¿¡åº¦ï¼šå¿…é¡»æœ‰æ ¸å¿ƒç»„ä»¶
            if not result['date_part']:
                result['confidence'] *= 0.5
                result['issues'].append("ç¼ºå°‘æ—¥æœŸä¿¡æ¯ä¸¥é‡å½±å“åŒ¹é…è´¨é‡")
            
            if not result['fuzzy_pattern_type']:
                result['confidence'] *= 0.5
                result['issues'].append("ç¼ºå°‘ç±»å‹æ¨¡å¼ä¸¥é‡å½±å“åŒ¹é…è´¨é‡")
            
            # ç½®ä¿¡åº¦è¯„çº§
            if result['confidence'] >= 0.8:
                result['quality'] = 'high'
            elif result['confidence'] >= 0.6:
                result['quality'] = 'medium'
            elif result['confidence'] >= 0.4:
                result['quality'] = 'low'
            else:
                result['quality'] = 'very_low'
                result['issues'].append("åŒ¹é…è´¨é‡è¿‡ä½ï¼Œå»ºè®®äººå·¥æ£€æŸ¥")
        else:
            result['issues'].append("ç¼ºå°‘æ ¸å¿ƒç»„ä»¶ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„æ¨¡ç³ŠåŒ¹é…")
        
        return result
    
    def get_known_locations(self):
        """ä»æ•°æ®é›†ä¸­æå–å·²çŸ¥ä½ç½®åç§°"""
        if not hasattr(self, '_known_locations'):
            locations = set()
            if self.df is not None:
                for _, row in self.df.iterrows():
                    filename = row['FileName']
                    # å°è¯•ä»å·²åŒ¹é…çš„æ–‡ä»¶ä¸­æå–ä½ç½®
                    exact_match = self.exact_pattern_match(filename)
                    if exact_match['is_exact_match'] and exact_match['location_part']:
                        locations.add(exact_match['location_part'])
            
            # æ·»åŠ ä¸€äº›å¸¸è§çš„åœ°ç†åç§°å˜ä½“
            common_locations = [
                'mahrous', 'mahmoud', 'mahros', 'mahroos',
                'taleh', 'tale', 'tal', 'tala',
                'jizi', 'jizy', 'gizi', 'gizy',
                'group3', 'group_3', 'grp3', 'g3'
            ]
            locations.update(common_locations)
            self._known_locations = list(locations)
        
        return self._known_locations
    
    def extract_location_candidates(self, filename: str, date_part: str, middle_part: str):
        """ä»æ–‡ä»¶åä¸­æå–ä½ç½®å€™é€‰åç§°"""
        candidates = []
        parts = filename.split('_')
        
        # åˆ›å»ºè¦æ’é™¤çš„è¯åˆ—è¡¨
        exclude_terms = set()
        if date_part:
            exclude_terms.add(date_part)
        
        # æ·»åŠ å¸¸è§çš„éä½ç½®è¯æ±‡
        common_non_location = {
            'finished', 'points', 'tracks', 'and', 'plan', 'route', 'routes', 
            'gmas', 'point', 'track', 'complete', 'done', 'planning', 'planned'
        }
        
        for part in parts:
            part_clean = part.strip()
            if not part_clean:
                continue
            
            # è·³è¿‡åŒ…å«æ—¥æœŸçš„éƒ¨åˆ†
            if date_part and date_part in part:
                continue
            
            # è·³è¿‡çº¯æ•°å­—
            if part_clean.isdigit():
                continue
            
            # è·³è¿‡å¸¸è§çš„éä½ç½®è¯æ±‡
            if part_clean.lower() in common_non_location:
                continue
            
            # è·³è¿‡åŒ…å«å¸¸è§å…³é”®è¯çš„éƒ¨åˆ†
            if any(term in part_clean.lower() for term in common_non_location):
                continue
            
            # ç§»é™¤æ‹¬å·å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦
            cleaned_part = re.sub(r'[^\w]', '', part_clean)
            if len(cleaned_part) >= 3:  # åªè€ƒè™‘é•¿åº¦>=3çš„å€™é€‰ä½ç½®
                candidates.append(cleaned_part)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å€™é€‰ä½ç½®ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªéƒ¨åˆ†
        if not candidates and parts:
            first_part = re.sub(r'[^\w]', '', parts[0])
            if len(first_part) >= 3:
                candidates.append(first_part)
        
        return candidates
    
    def analyze_dataset(self):
        """åˆ†ææ•´ä¸ªæ•°æ®é›†"""
        if self.df is None:
            print("è¯·å…ˆåŠ è½½æ•°æ®")
            return None
        
        analysis_results = {
            'total_files': len(self.df),
            'date_analysis': {},
            'pattern_analysis': {},
            'location_analysis': {},
            'file_size_analysis': {},
            'temporal_analysis': {},
            'detailed_files': []
        }
        
        # æ·»åŠ åˆ†æåˆ—
        date_info_list = []
        pattern_info_list = []
        exact_match_list = []
        fuzzy_match_list = []
        
        for _, row in self.df.iterrows():
            filename = row['FileName']
            
            # æå–æ—¥æœŸä¿¡æ¯
            date_info = self.extract_date_patterns(filename)
            date_info_list.append(date_info)
            
            # æå–æ¨¡å¼ä¿¡æ¯
            pattern_info = self.extract_name_patterns(filename)
            pattern_info_list.append(pattern_info)
            
            # ç²¾ç¡®åŒ¹é…åˆ†æ
            exact_match_info = self.exact_pattern_match(filename)
            exact_match_list.append(exact_match_info)
            
            # æ¨¡ç³ŠåŒ¹é…åˆ†æï¼ˆä»…å¯¹æ— æ³•ç²¾ç¡®åŒ¹é…çš„æ–‡ä»¶ï¼‰
            if not exact_match_info['is_exact_match']:
                fuzzy_match_info = self.fuzzy_pattern_match(filename)
                fuzzy_match_list.append(fuzzy_match_info)
            else:
                # ç²¾ç¡®åŒ¹é…çš„æ–‡ä»¶ä¸éœ€è¦æ¨¡ç³ŠåŒ¹é…
                fuzzy_match_list.append({
                    'is_fuzzy_match': False,
                    'location_part': None,
                    'middle_part': None,
                    'date_part': None,
                    'fuzzy_pattern_type': None,
                    'confidence': 0.0,
                    'issues': []
                })
            
            # è¯¦ç»†æ–‡ä»¶ä¿¡æ¯
            file_detail = {
                'filename': filename,
                'directory': row['Directory'],
                'size': row['Size'],
                'date_info': date_info,
                'pattern_info': pattern_info,
                'exact_match_info': exact_match_info,
                'fuzzy_match_info': fuzzy_match_list[-1]
            }
            analysis_results['detailed_files'].append(file_detail)
        
        # æ—¥æœŸåˆ†æ
        years = [info['year'] for info in date_info_list if info['year']]
        months = [info['month'] for info in date_info_list if info['month']]
        dates_8digit = [info['date_8digit'] for info in date_info_list if info['date_8digit']]
        
        analysis_results['date_analysis'] = {
            'files_with_dates': len([d for d in date_info_list if d['date_8digit'] or d['date_6digit']]),
            'year_distribution': dict(Counter(years)),
            'month_distribution': dict(Counter(months)),
            'date_range': {
                'earliest': min(dates_8digit) if dates_8digit else None,
                'latest': max(dates_8digit) if dates_8digit else None
            }
        }
        
        # æ¨¡å¼åˆ†æ
        pattern_types = [info['pattern_type'] for info in pattern_info_list]
        location_names = [info['location_name'] for info in pattern_info_list if info['location_name']]
        exact_match_types = [info['exact_pattern_type'] for info in exact_match_list if info['is_exact_match']]
        fuzzy_match_types = [info['fuzzy_pattern_type'] for info in fuzzy_match_list if info['is_fuzzy_match']]
        
        # ç»Ÿè®¡æ¨¡ç³ŠåŒ¹é…çš„ç½®ä¿¡åº¦
        fuzzy_confidences = [info['confidence'] for info in fuzzy_match_list if info['is_fuzzy_match']]
        avg_fuzzy_confidence = sum(fuzzy_confidences) / len(fuzzy_confidences) if fuzzy_confidences else 0
        
        # ç»Ÿè®¡æ— æ³•åŒ¹é…çš„æ–‡ä»¶
        unmatched_files = [
            i for i, (exact, fuzzy) in enumerate(zip(exact_match_list, fuzzy_match_list))
            if not exact['is_exact_match'] and not fuzzy['is_fuzzy_match']
        ]
        
        analysis_results['pattern_analysis'] = {
            'pattern_type_distribution': dict(Counter(pattern_types)),
            'finished_points_and_tracks_files': len([p for p in pattern_info_list if p['has_finished_points_and_tracks']]),
            'plan_routes_files': len([p for p in pattern_info_list if p['has_plan_routes']]),
            'exact_match_analysis': {
                'total_exact_matches': len([e for e in exact_match_list if e['is_exact_match']]),
                'exact_match_types': dict(Counter(exact_match_types)),
                'exact_match_rate': len([e for e in exact_match_list if e['is_exact_match']]) / len(exact_match_list) * 100
            },
            'fuzzy_match_analysis': {
                'total_fuzzy_matches': len([f for f in fuzzy_match_list if f['is_fuzzy_match']]),
                'fuzzy_match_types': dict(Counter(fuzzy_match_types)),
                'average_confidence': avg_fuzzy_confidence,
                'high_confidence_matches': len([f for f in fuzzy_match_list if f['is_fuzzy_match'] and f['confidence'] >= 0.7]),
                'low_confidence_matches': len([f for f in fuzzy_match_list if f['is_fuzzy_match'] and f['confidence'] < 0.5])
            },
            'total_coverage': {
                'exact_matches': len([e for e in exact_match_list if e['is_exact_match']]),
                'fuzzy_matches': len([f for f in fuzzy_match_list if f['is_fuzzy_match']]),
                'unmatched': len(unmatched_files),
                'total_coverage_rate': (len([e for e in exact_match_list if e['is_exact_match']]) + 
                                      len([f for f in fuzzy_match_list if f['is_fuzzy_match']])) / len(exact_match_list) * 100
            }
        }
        
        analysis_results['location_analysis'] = {
            'unique_locations': len(set(location_names)),
            'location_frequency': dict(Counter(location_names).most_common(20))  # å‰20ä¸ªæœ€é¢‘ç¹çš„ä½ç½®
        }
        
        # æ–‡ä»¶å¤§å°åˆ†æ
        sizes = self.df['Size'].astype(int)
        analysis_results['file_size_analysis'] = {
            'total_size_mb': sizes.sum() / (1024 * 1024),
            'average_size_kb': sizes.mean() / 1024,
            'size_range': {
                'min_kb': sizes.min() / 1024,
                'max_kb': sizes.max() / 1024
            }
        }
        
        # æ—¶é—´åˆ†æï¼ˆåŸºäºç›®å½•ï¼‰
        directories = self.df['Directory'].value_counts()
        analysis_results['temporal_analysis'] = {
            'directory_distribution': directories.to_dict(),
            'most_active_periods': directories.head(10).to_dict()
        }
        
        return analysis_results
    
    def print_analysis_summary(self, results):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        if not results:
            return
        
        print("\n" + "="*80)
        print("KMZæ–‡ä»¶æ•°æ®é›†åˆ†ææŠ¥å‘Š")
        print("="*80)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶æ•°: {results['total_files']:,}")
        print(f"   æ€»å¤§å°: {results['file_size_analysis']['total_size_mb']:.1f} MB")
        print(f"   å¹³å‡æ–‡ä»¶å¤§å°: {results['file_size_analysis']['average_size_kb']:.1f} KB")
        
        print(f"\nğŸ“… æ—¥æœŸåˆ†æ:")
        print(f"   åŒ…å«æ—¥æœŸçš„æ–‡ä»¶: {results['date_analysis']['files_with_dates']}")
        print(f"   å¹´ä»½åˆ†å¸ƒ: {results['date_analysis']['year_distribution']}")
        print(f"   æ—¥æœŸèŒƒå›´: {results['date_analysis']['date_range']['earliest']} ~ {results['date_analysis']['date_range']['latest']}")
        
        print(f"\nğŸ·ï¸ æ–‡ä»¶æ¨¡å¼åˆ†æ:")
        for pattern, count in results['pattern_analysis']['pattern_type_distribution'].items():
            print(f"   {pattern}: {count} ä¸ªæ–‡ä»¶")
        
        print(f"\nğŸ¯ ç²¾ç¡®åŒ¹é…åˆ†æ:")
        exact_analysis = results['pattern_analysis']['exact_match_analysis']
        print(f"   ç²¾ç¡®åŒ¹é…æ–‡ä»¶æ•°: {exact_analysis['total_exact_matches']}")
        print(f"   ç²¾ç¡®åŒ¹é…ç‡: {exact_analysis['exact_match_rate']:.1f}%")
        print(f"   ç²¾ç¡®åŒ¹é…ç±»å‹åˆ†å¸ƒ:")
        for match_type, count in exact_analysis['exact_match_types'].items():
            print(f"     {match_type}: {count} ä¸ªæ–‡ä»¶")
        
        print(f"\nğŸ” æ¨¡ç³ŠåŒ¹é…åˆ†æ:")
        fuzzy_analysis = results['pattern_analysis']['fuzzy_match_analysis']
        total_coverage = results['pattern_analysis']['total_coverage']
        print(f"   æ¨¡ç³ŠåŒ¹é…æ–‡ä»¶æ•°: {fuzzy_analysis['total_fuzzy_matches']}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {fuzzy_analysis['average_confidence']:.2f}")
        print(f"   é«˜ç½®ä¿¡åº¦åŒ¹é… (â‰¥0.7): {fuzzy_analysis['high_confidence_matches']} ä¸ªæ–‡ä»¶")
        print(f"   ä½ç½®ä¿¡åº¦åŒ¹é… (<0.5): {fuzzy_analysis['low_confidence_matches']} ä¸ªæ–‡ä»¶")
        print(f"   æ¨¡ç³ŠåŒ¹é…ç±»å‹åˆ†å¸ƒ:")
        for match_type, count in fuzzy_analysis['fuzzy_match_types'].items():
            print(f"     {match_type}: {count} ä¸ªæ–‡ä»¶")
        
        print(f"\nğŸ“ˆ æ€»ä½“è¦†ç›–ç‡åˆ†æ:")
        print(f"   ç²¾ç¡®åŒ¹é…: {total_coverage['exact_matches']} ä¸ªæ–‡ä»¶")
        print(f"   æ¨¡ç³ŠåŒ¹é…: {total_coverage['fuzzy_matches']} ä¸ªæ–‡ä»¶")
        print(f"   æ— æ³•åŒ¹é…: {total_coverage['unmatched']} ä¸ªæ–‡ä»¶")
        print(f"   æ€»è¦†ç›–ç‡: {total_coverage['total_coverage_rate']:.1f}%")
        
        print(f"\nğŸ“ ä½ç½®åˆ†æ:")
        print(f"   å”¯ä¸€ä½ç½®æ•°: {results['location_analysis']['unique_locations']}")
        print(f"   æœ€é¢‘ç¹çš„ä½ç½®:")
        for location, count in list(results['location_analysis']['location_frequency'].items())[:10]:
            print(f"     {location}: {count} ä¸ªæ–‡ä»¶")
        
        print(f"\nğŸ“ æ—¶é—´åˆ†å¸ƒåˆ†æ:")
        print(f"   æœ€æ´»è·ƒçš„æ—¶æœŸ:")
        for period, count in list(results['temporal_analysis']['most_active_periods'].items())[:10]:
            print(f"     {period}: {count} ä¸ªæ–‡ä»¶")
    
    def save_analysis_results(self, results, output_file):
        """ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nâœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
    
    def show_unmatched_files(self, results):
        """æ˜¾ç¤ºæ— æ³•åŒ¹é…çš„æ–‡ä»¶è¯¦æƒ…"""
        unmatched_files = []
        
        for file_detail in results['detailed_files']:
            exact_match = file_detail['exact_match_info']
            fuzzy_match = file_detail['fuzzy_match_info']
            
            if not exact_match['is_exact_match'] and not fuzzy_match['is_fuzzy_match']:
                unmatched_files.append({
                    'filename': file_detail['filename'],
                    'directory': file_detail['directory'],
                    'fuzzy_issues': fuzzy_match['issues']
                })
        
        if unmatched_files:
            print(f"\nâŒ æ— æ³•åŒ¹é…çš„æ–‡ä»¶ ({len(unmatched_files)}):")
            for i, file_info in enumerate(unmatched_files[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   {i}. {file_info['filename']}")
                if file_info['fuzzy_issues']:
                    print(f"      é—®é¢˜: {', '.join(file_info['fuzzy_issues'])}")
            
            if len(unmatched_files) > 10:
                print(f"   ... è¿˜æœ‰ {len(unmatched_files) - 10} ä¸ªæ— æ³•åŒ¹é…çš„æ–‡ä»¶")
        else:
            print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶éƒ½èƒ½å¤ŸæˆåŠŸåŒ¹é…ï¼")
    
    def show_fuzzy_matched_files(self, results):
        """æ˜¾ç¤ºæ¨¡ç³ŠåŒ¹é…æ–‡ä»¶çš„è¯¦æƒ…"""
        fuzzy_matched_files = []
        
        for file_detail in results['detailed_files']:
            exact_match = file_detail['exact_match_info']
            fuzzy_match = file_detail['fuzzy_match_info']
            
            if not exact_match['is_exact_match'] and fuzzy_match['is_fuzzy_match']:
                fuzzy_matched_files.append({
                    'filename': file_detail['filename'],
                    'directory': file_detail['directory'],
                    'size': file_detail['size'],
                    'fuzzy_info': fuzzy_match
                })
        
        if fuzzy_matched_files:
            print(f"\nğŸ” æ¨¡ç³ŠåŒ¹é…æ–‡ä»¶è¯¦æƒ… ({len(fuzzy_matched_files)}):")
            for i, file_info in enumerate(fuzzy_matched_files, 1):
                fuzzy = file_info['fuzzy_info']
                print(f"\n   {i}. {file_info['filename']}")
                print(f"      ç›®å½•: {file_info['directory']}")
                print(f"      å¤§å°: {file_info['size']/1024:.1f} KB")
                print(f"      åŒ¹é…ç±»å‹: {fuzzy['fuzzy_pattern_type']}")
                print(f"      ç½®ä¿¡åº¦: {fuzzy['confidence']:.2f}")
                print(f"      ä½ç½®éƒ¨åˆ†: {fuzzy['location_part']}")
                print(f"      ä¸­é—´éƒ¨åˆ†: {fuzzy['middle_part']}")
                print(f"      æ—¥æœŸéƒ¨åˆ†: {fuzzy['date_part']}")
                if fuzzy['issues']:
                    print(f"      é—®é¢˜: {', '.join(fuzzy['issues'])}")
        else:
            print(f"\nâœ… æ²¡æœ‰éœ€è¦æ¨¡ç³ŠåŒ¹é…çš„æ–‡ä»¶ï¼")
    
    def extract_finished_points_files(self, results):
        """æå–ç¬¦åˆ'finished_points_and_tracks'æ¨¡å¼çš„æ–‡ä»¶"""
        finished_points_files = []
        
        for file_detail in results['detailed_files']:
            if file_detail['pattern_info']['has_finished_points_and_tracks']:
                finished_points_files.append({
                    'filename': file_detail['filename'],
                    'location': file_detail['pattern_info']['location_name'],
                    'date': file_detail['date_info']['date_8digit'],
                    'directory': file_detail['directory']
                })
        
        print(f"\nğŸ¯ ç¬¦åˆ'finished_points_and_tracks'æ¨¡å¼çš„æ–‡ä»¶: {len(finished_points_files)}")
        
        # æŒ‰ä½ç½®åˆ†ç»„
        by_location = defaultdict(list)
        for file in finished_points_files:
            by_location[file['location']].append(file)
        
        print(f"   æ¶‰åŠä½ç½®æ•°: {len(by_location)}")
        for location, files in sorted(by_location.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
            print(f"     {location}: {len(files)} ä¸ªæ–‡ä»¶")
        
        return finished_points_files

def main():
    """ä¸»å‡½æ•°"""
    csv_file = "D:\\MacBook\\MacBookDocument\\SourceCode\\GitHub_Public_Repos\\GMAS_Script\\DailyDataCollection\\core\\utils\\matcher\\string_matching\\tests\\test_data\\kmz_filename\\kmz_files_dataset.csv"
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = KMZDatasetAnalyzer(csv_file)
    
    # åŠ è½½æ•°æ®
    if not analyzer.load_data():
        return
    
    # æ‰§è¡Œåˆ†æ
    print("æ­£åœ¨åˆ†æKMZæ–‡ä»¶æ•°æ®é›†...")
    results = analyzer.analyze_dataset()
    
    if results:
        # æ‰“å°æ‘˜è¦
        analyzer.print_analysis_summary(results)
        
        # æå–finished_pointsæ–‡ä»¶
        finished_points_files = analyzer.extract_finished_points_files(results)
        
        # ä¿å­˜å®Œæ•´åˆ†æç»“æœ
        output_file = "D:\\MacBook\\MacBookDocument\\SourceCode\\GitHub_Public_Repos\\GMAS_Script\\DailyDataCollection\\core\\utils\\matcher\\string_matching\\tests\\test_data\\kmz_filename\\kmz_filename_kmz_analysis_results.json"
        analyzer.save_analysis_results(results, output_file)
        
        # ä¿å­˜finished_pointsæ–‡ä»¶åˆ—è¡¨
        finished_points_file = "D:\\MacBook\\MacBookDocument\\SourceCode\\GitHub_Public_Repos\\GMAS_Script\\DailyDataCollection\\core\\utils\\matcher\\string_matching\\tests\\test_data\\kmz_filename\\finished_points_files.json"
        try:
            with open(finished_points_file, 'w', encoding='utf-8') as f:
                json.dump(finished_points_files, f, ensure_ascii=False, indent=2)
            print(f"âœ… finished_pointsæ–‡ä»¶åˆ—è¡¨å·²ä¿å­˜åˆ°: {finished_points_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜finished_pointsæ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºæ— æ³•åŒ¹é…çš„æ–‡ä»¶
        analyzer.show_unmatched_files(results)
        
        # æ˜¾ç¤ºæ¨¡ç³ŠåŒ¹é…æ–‡ä»¶çš„è¯¦æƒ…
        analyzer.show_fuzzy_matched_files(results)

if __name__ == "__main__":
    main()
