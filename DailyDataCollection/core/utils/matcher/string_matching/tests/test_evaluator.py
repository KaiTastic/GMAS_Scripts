#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»£ç è¯„ä¼°å™¨ - å¯¹å¤šç›®æ ‡å­—ç¬¦ä¸²åŒ¹é…å™¨è¿›è¡Œå…¨é¢çš„ä»£ç è´¨é‡è¯„ä¼°
"""

import ast
import os
import time
import inspect
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import json
from pathlib import Path


class QualityLevel(Enum):
    """ä»£ç è´¨é‡ç­‰çº§"""
    EXCELLENT = "ä¼˜ç§€"
    GOOD = "è‰¯å¥½"
    FAIR = "ä¸€èˆ¬"
    POOR = "è¾ƒå·®"
    CRITICAL = "ä¸¥é‡"


@dataclass
class CodeMetrics:
    """ä»£ç æŒ‡æ ‡"""
    lines_of_code: int
    lines_of_comments: int
    blank_lines: int
    cyclomatic_complexity: int
    functions_count: int
    classes_count: int
    imports_count: int
    docstring_coverage: float
    test_coverage: float = 0.0


@dataclass
class QualityIssue:
    """è´¨é‡é—®é¢˜"""
    severity: QualityLevel
    category: str
    description: str
    file_path: str
    line_number: int = 0
    suggestion: str = ""


class TestEvaluator:
    """æµ‹è¯•è¯„ä¼°å™¨
    
    å¯¹ä»£ç è¿›è¡Œå¤šç»´åº¦çš„è´¨é‡è¯„ä¼°
    """
    
    def __init__(self, project_root: str):
        """åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•
        """
        self.project_root = Path(project_root)
        self.code_metrics: Dict[str, CodeMetrics] = {}
        self.quality_issues: List[QualityIssue] = []
        self.evaluation_results: Dict[str, Any] = {}
        
    def evaluate_project(self) -> Dict[str, Any]:
        """è¯„ä¼°æ•´ä¸ªé¡¹ç›®
        
        Returns:
            Dict[str, Any]: è¯„ä¼°ç»“æœ
        """
        print("ğŸ” å¼€å§‹ä»£ç è´¨é‡è¯„ä¼°...")
        
        # 1. æ”¶é›†ä»£ç æŒ‡æ ‡
        self._collect_code_metrics()
        
        # 2. åˆ†æä»£ç è´¨é‡é—®é¢˜
        self._analyze_quality_issues()
        
        # 3. è¯„ä¼°æµ‹è¯•è¦†ç›–ç‡
        self._evaluate_test_coverage()
        
        # 4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        report = self._generate_evaluation_report()
        
        self.evaluation_results = report
        return report
    
    def _collect_code_metrics(self):
        """æ”¶é›†ä»£ç æŒ‡æ ‡"""
        print("ğŸ“Š æ”¶é›†ä»£ç æŒ‡æ ‡...")
        
        python_files = list(self.project_root.glob("**/*.py"))
        
        for file_path in python_files:
            if self._should_analyze_file(file_path):
                metrics = self._analyze_file_metrics(file_path)
                self.code_metrics[str(file_path)] = metrics
    
    def _should_analyze_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ†ææ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åˆ†æ
        """
        # è·³è¿‡ç¼“å­˜æ–‡ä»¶å’Œæµ‹è¯•æ•°æ®
        exclude_patterns = ['__pycache__', '.pyc', 'test_data', '.git']
        
        for pattern in exclude_patterns:
            if pattern in str(file_path):
                return False
        
        return True
    
    def _analyze_file_metrics(self, file_path: Path) -> CodeMetrics:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„æŒ‡æ ‡
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            CodeMetrics: æ–‡ä»¶æŒ‡æ ‡
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.splitlines()
            total_lines = len(lines)
            
            # è®¡ç®—æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
            comment_lines = 0
            blank_lines = 0
            
            for line in lines:
                stripped = line.strip()
                if not stripped:
                    blank_lines += 1
                elif stripped.startswith('#'):
                    comment_lines += 1
            
            code_lines = total_lines - comment_lines - blank_lines
            
            # è§£æAST
            try:
                tree = ast.parse(content)
                
                functions_count = len([n for n in ast.walk(tree) 
                                     if isinstance(n, ast.FunctionDef)])
                classes_count = len([n for n in ast.walk(tree) 
                                   if isinstance(n, ast.ClassDef)])
                imports_count = len([n for n in ast.walk(tree) 
                                   if isinstance(n, (ast.Import, ast.ImportFrom))])
                
                # è®¡ç®—åœˆå¤æ‚åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
                complexity = self._calculate_complexity(tree)
                
                # è®¡ç®—æ–‡æ¡£å­—ç¬¦ä¸²è¦†ç›–ç‡
                docstring_coverage = self._calculate_docstring_coverage(tree)
                
            except SyntaxError:
                # è¯­æ³•é”™è¯¯æ—¶ä½¿ç”¨é»˜è®¤å€¼
                functions_count = 0
                classes_count = 0
                imports_count = 0
                complexity = 1
                docstring_coverage = 0.0
            
            return CodeMetrics(
                lines_of_code=code_lines,
                lines_of_comments=comment_lines,
                blank_lines=blank_lines,
                cyclomatic_complexity=complexity,
                functions_count=functions_count,
                classes_count=classes_count,
                imports_count=imports_count,
                docstring_coverage=docstring_coverage
            )
            
        except Exception as e:
            print(f"è­¦å‘Š: åˆ†ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return CodeMetrics(0, 0, 0, 1, 0, 0, 0, 0.0)
    
    def _calculate_complexity(self, tree: ast.AST) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦
        
        Args:
            tree: ASTæ ‘
            
        Returns:
            int: åœˆå¤æ‚åº¦
        """
        complexity = 1  # åŸºç¡€å¤æ‚åº¦
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor,
                               ast.ExceptHandler, ast.With, ast.AsyncWith)):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1
        
        return complexity
    
    def _calculate_docstring_coverage(self, tree: ast.AST) -> float:
        """è®¡ç®—æ–‡æ¡£å­—ç¬¦ä¸²è¦†ç›–ç‡
        
        Args:
            tree: ASTæ ‘
            
        Returns:
            float: è¦†ç›–ç‡ç™¾åˆ†æ¯”
        """
        documentable_items = []
        documented_items = []
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                documentable_items.append(node)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£å­—ç¬¦ä¸²
                if (node.body and isinstance(node.body[0], ast.Expr) and 
                    isinstance(node.body[0].value, ast.Constant) and 
                    isinstance(node.body[0].value.value, str)):
                    documented_items.append(node)
        
        if not documentable_items:
            return 100.0  # æ²¡æœ‰å¯æ–‡æ¡£åŒ–çš„é¡¹ç›®
        
        return (len(documented_items) / len(documentable_items)) * 100
    
    def _analyze_quality_issues(self):
        """åˆ†æä»£ç è´¨é‡é—®é¢˜"""
        print("ğŸ” åˆ†æä»£ç è´¨é‡é—®é¢˜...")
        
        for file_path, metrics in self.code_metrics.items():
            self._check_file_quality(file_path, metrics)
    
    def _check_file_quality(self, file_path: str, metrics: CodeMetrics):
        """æ£€æŸ¥å•ä¸ªæ–‡ä»¶çš„è´¨é‡é—®é¢˜
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            metrics: æ–‡ä»¶æŒ‡æ ‡
        """
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if metrics.lines_of_code > 500:
            self.quality_issues.append(QualityIssue(
                severity=QualityLevel.FAIR,
                category="ä»£ç ç»“æ„",
                description=f"æ–‡ä»¶è¿‡å¤§: {metrics.lines_of_code} è¡Œä»£ç ",
                file_path=file_path,
                suggestion="è€ƒè™‘å°†å¤§æ–‡ä»¶æ‹†åˆ†ä¸ºå¤šä¸ªå°æ–‡ä»¶"
            ))
        
        # æ£€æŸ¥å¤æ‚åº¦
        if metrics.cyclomatic_complexity > 20:
            self.quality_issues.append(QualityIssue(
                severity=QualityLevel.POOR,
                category="ä»£ç å¤æ‚åº¦",
                description=f"åœˆå¤æ‚åº¦è¿‡é«˜: {metrics.cyclomatic_complexity}",
                file_path=file_path,
                suggestion="ç®€åŒ–é€»è¾‘ç»“æ„ï¼Œæ‹†åˆ†å¤æ‚å‡½æ•°"
            ))
        elif metrics.cyclomatic_complexity > 10:
            self.quality_issues.append(QualityIssue(
                severity=QualityLevel.FAIR,
                category="ä»£ç å¤æ‚åº¦",
                description=f"åœˆå¤æ‚åº¦è¾ƒé«˜: {metrics.cyclomatic_complexity}",
                file_path=file_path,
                suggestion="è€ƒè™‘é‡æ„å¤æ‚çš„é€»è¾‘ç»“æ„"
            ))
        
        # æ£€æŸ¥æ–‡æ¡£è¦†ç›–ç‡
        if metrics.docstring_coverage < 50:
            self.quality_issues.append(QualityIssue(
                severity=QualityLevel.FAIR,
                category="æ–‡æ¡£è´¨é‡",
                description=f"æ–‡æ¡£è¦†ç›–ç‡ä½: {metrics.docstring_coverage:.1f}%",
                file_path=file_path,
                suggestion="å¢åŠ å‡½æ•°å’Œç±»çš„æ–‡æ¡£å­—ç¬¦ä¸²"
            ))
        
        # æ£€æŸ¥æ³¨é‡Šæ¯”ä¾‹
        total_lines = metrics.lines_of_code + metrics.lines_of_comments + metrics.blank_lines
        if total_lines > 0:
            comment_ratio = metrics.lines_of_comments / total_lines
            if comment_ratio < 0.1 and metrics.lines_of_code > 50:
                self.quality_issues.append(QualityIssue(
                    severity=QualityLevel.FAIR,
                    category="ä»£ç å¯è¯»æ€§",
                    description=f"æ³¨é‡Šè¾ƒå°‘: {comment_ratio:.1%}",
                    file_path=file_path,
                    suggestion="å¢åŠ å¿…è¦çš„æ³¨é‡Šä»¥æé«˜ä»£ç å¯è¯»æ€§"
                ))
    
    def _evaluate_test_coverage(self):
        """è¯„ä¼°æµ‹è¯•è¦†ç›–ç‡"""
        print("ğŸ§ª è¯„ä¼°æµ‹è¯•è¦†ç›–ç‡...")
        
        # ç»Ÿè®¡æµ‹è¯•æ–‡ä»¶å’Œæºä»£ç æ–‡ä»¶
        test_files = []
        source_files = []
        
        for file_path in self.code_metrics.keys():
            path = Path(file_path)
            if 'test' in path.name or 'tests' in str(path):
                test_files.append(file_path)
            else:
                source_files.append(file_path)
        
        # è®¡ç®—æµ‹è¯•è¦†ç›–ç‡ï¼ˆç®€åŒ–ä¼°ç®—ï¼‰
        if source_files:
            test_ratio = len(test_files) / len(source_files)
            avg_test_coverage = min(test_ratio * 100, 100)
            
            # æ›´æ–°æ‰€æœ‰æ–‡ä»¶çš„æµ‹è¯•è¦†ç›–ç‡
            for file_path in self.code_metrics:
                self.code_metrics[file_path].test_coverage = avg_test_coverage
            
            # æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡
            if avg_test_coverage < 50:
                self.quality_issues.append(QualityIssue(
                    severity=QualityLevel.POOR,
                    category="æµ‹è¯•è´¨é‡",
                    description=f"æµ‹è¯•è¦†ç›–ç‡ä¸è¶³: {avg_test_coverage:.1f}%",
                    file_path="é¡¹ç›®æ•´ä½“",
                    suggestion="å¢åŠ æ›´å¤šçš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•"
                ))
    
    def _generate_evaluation_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Returns:
            Dict[str, Any]: è¯„ä¼°æŠ¥å‘Š
        """
        print("ğŸ“‹ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        # æ±‡æ€»ç»Ÿè®¡
        total_metrics = self._aggregate_metrics()
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»é—®é¢˜
        issues_by_severity = {}
        for issue in self.quality_issues:
            severity = issue.severity.value
            if severity not in issues_by_severity:
                issues_by_severity[severity] = []
            issues_by_severity[severity].append(issue)
        
        # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
        quality_score = self._calculate_quality_score(total_metrics, issues_by_severity)
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(issues_by_severity)
        
        report = {
            'evaluation_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'project_metrics': {
                'total_files': len(self.code_metrics),
                'total_lines_of_code': total_metrics.lines_of_code,
                'total_lines_of_comments': total_metrics.lines_of_comments,
                'total_functions': total_metrics.functions_count,
                'total_classes': total_metrics.classes_count,
                'average_complexity': total_metrics.cyclomatic_complexity / len(self.code_metrics) if self.code_metrics else 0,
                'average_docstring_coverage': total_metrics.docstring_coverage / len(self.code_metrics) if self.code_metrics else 0,
                'estimated_test_coverage': total_metrics.test_coverage
            },
            'quality_score': quality_score,
            'quality_level': self._get_quality_level(quality_score),
            'issues_summary': {
                'total_issues': len(self.quality_issues),
                'by_severity': {severity: len(issues) for severity, issues in issues_by_severity.items()}
            },
            'detailed_issues': [
                {
                    'severity': issue.severity.value,
                    'category': issue.category,
                    'description': issue.description,
                    'file': os.path.basename(issue.file_path),
                    'line': issue.line_number,
                    'suggestion': issue.suggestion
                }
                for issue in self.quality_issues
            ],
            'recommendations': recommendations,
            'file_metrics': {
                os.path.basename(path): {
                    'lines_of_code': metrics.lines_of_code,
                    'complexity': metrics.cyclomatic_complexity,
                    'docstring_coverage': metrics.docstring_coverage,
                    'functions': metrics.functions_count,
                    'classes': metrics.classes_count
                }
                for path, metrics in self.code_metrics.items()
            }
        }
        
        return report
    
    def _aggregate_metrics(self) -> CodeMetrics:
        """æ±‡æ€»æ‰€æœ‰æ–‡ä»¶çš„æŒ‡æ ‡
        
        Returns:
            CodeMetrics: æ±‡æ€»æŒ‡æ ‡
        """
        if not self.code_metrics:
            return CodeMetrics(0, 0, 0, 0, 0, 0, 0, 0.0, 0.0)
        
        total_loc = sum(m.lines_of_code for m in self.code_metrics.values())
        total_comments = sum(m.lines_of_comments for m in self.code_metrics.values())
        total_blank = sum(m.blank_lines for m in self.code_metrics.values())
        total_complexity = sum(m.cyclomatic_complexity for m in self.code_metrics.values())
        total_functions = sum(m.functions_count for m in self.code_metrics.values())
        total_classes = sum(m.classes_count for m in self.code_metrics.values())
        total_imports = sum(m.imports_count for m in self.code_metrics.values())
        
        avg_docstring = sum(m.docstring_coverage for m in self.code_metrics.values()) / len(self.code_metrics)
        avg_test_coverage = sum(m.test_coverage for m in self.code_metrics.values()) / len(self.code_metrics)
        
        return CodeMetrics(
            lines_of_code=total_loc,
            lines_of_comments=total_comments,
            blank_lines=total_blank,
            cyclomatic_complexity=total_complexity,
            functions_count=total_functions,
            classes_count=total_classes,
            imports_count=total_imports,
            docstring_coverage=avg_docstring,
            test_coverage=avg_test_coverage
        )
    
    def _calculate_quality_score(self, metrics: CodeMetrics, 
                                issues_by_severity: Dict[str, List]) -> float:
        """è®¡ç®—è´¨é‡åˆ†æ•°
        
        Args:
            metrics: ä»£ç æŒ‡æ ‡
            issues_by_severity: æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»çš„é—®é¢˜
            
        Returns:
            float: è´¨é‡åˆ†æ•° (0-100)
        """
        base_score = 100.0
        
        # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        severity_weights = {
            QualityLevel.CRITICAL.value: 20,
            QualityLevel.POOR.value: 10,
            QualityLevel.FAIR.value: 5,
            QualityLevel.GOOD.value: 2,
            QualityLevel.EXCELLENT.value: 0
        }
        
        for severity, issues in issues_by_severity.items():
            weight = severity_weights.get(severity, 5)
            base_score -= len(issues) * weight
        
        # æ ¹æ®è¦†ç›–ç‡è°ƒæ•´åˆ†æ•°
        if metrics.docstring_coverage < 50:
            base_score -= 10
        if metrics.test_coverage < 50:
            base_score -= 15
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        return max(0, min(100, base_score))
    
    def _get_quality_level(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è·å–è´¨é‡ç­‰çº§
        
        Args:
            score: è´¨é‡åˆ†æ•°
            
        Returns:
            str: è´¨é‡ç­‰çº§
        """
        if score >= 90:
            return QualityLevel.EXCELLENT.value
        elif score >= 80:
            return QualityLevel.GOOD.value
        elif score >= 60:
            return QualityLevel.FAIR.value
        elif score >= 40:
            return QualityLevel.POOR.value
        else:
            return QualityLevel.CRITICAL.value
    
    def _generate_recommendations(self, issues_by_severity: Dict[str, List]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®
        
        Args:
            issues_by_severity: æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»çš„é—®é¢˜
            
        Returns:
            List[str]: å»ºè®®åˆ—è¡¨
        """
        recommendations = []
        
        # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆå»ºè®®
        issue_categories = {}
        for issues in issues_by_severity.values():
            for issue in issues:
                category = issue.category
                if category not in issue_categories:
                    issue_categories[category] = 0
                issue_categories[category] += 1
        
        # ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
        if issue_categories.get('ä»£ç å¤æ‚åº¦', 0) > 0:
            recommendations.append("ç®€åŒ–å¤æ‚çš„å‡½æ•°å’Œç±»ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™")
        
        if issue_categories.get('æ–‡æ¡£è´¨é‡', 0) > 0:
            recommendations.append("å¢åŠ å‡½æ•°ã€ç±»å’Œæ¨¡å—çš„æ–‡æ¡£å­—ç¬¦ä¸²")
        
        if issue_categories.get('æµ‹è¯•è´¨é‡', 0) > 0:
            recommendations.append("ç¼–å†™æ›´å¤šçš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•")
        
        if issue_categories.get('ä»£ç ç»“æ„', 0) > 0:
            recommendations.append("é‡æ„å¤§æ–‡ä»¶ï¼Œæé«˜ä»£ç çš„æ¨¡å—åŒ–ç¨‹åº¦")
        
        if issue_categories.get('ä»£ç å¯è¯»æ€§', 0) > 0:
            recommendations.append("å¢åŠ å¿…è¦çš„æ³¨é‡Šï¼Œæé«˜ä»£ç å¯è¯»æ€§")
        
        if not recommendations:
            recommendations.append("ä»£ç è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
        
        return recommendations
    
    def print_evaluation_report(self):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        if not self.evaluation_results:
            print("é”™è¯¯: æ²¡æœ‰å¯æ˜¾ç¤ºçš„è¯„ä¼°ç»“æœ")
            return
        
        report = self.evaluation_results
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š ä»£ç è´¨é‡è¯„ä¼°æŠ¥å‘Š")
        print(f"{'='*70}")
        print(f"è¯„ä¼°æ—¶é—´: {report['evaluation_time']}")
        print(f"è´¨é‡åˆ†æ•°: {report['quality_score']:.1f}/100")
        print(f"è´¨é‡ç­‰çº§: {report['quality_level']}")
        
        print(f"\nğŸ“ˆ é¡¹ç›®æŒ‡æ ‡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {report['project_metrics']['total_files']}")
        print(f"  æ€»ä»£ç è¡Œæ•°: {report['project_metrics']['total_lines_of_code']}")
        print(f"  æ€»æ³¨é‡Šè¡Œæ•°: {report['project_metrics']['total_lines_of_comments']}")
        print(f"  å‡½æ•°æ€»æ•°: {report['project_metrics']['total_functions']}")
        print(f"  ç±»æ€»æ•°: {report['project_metrics']['total_classes']}")
        print(f"  å¹³å‡å¤æ‚åº¦: {report['project_metrics']['average_complexity']:.1f}")
        print(f"  æ–‡æ¡£è¦†ç›–ç‡: {report['project_metrics']['average_docstring_coverage']:.1f}%")
        print(f"  æµ‹è¯•è¦†ç›–ç‡: {report['project_metrics']['estimated_test_coverage']:.1f}%")
        
        print(f"\nâš ï¸  è´¨é‡é—®é¢˜æ±‡æ€»:")
        print(f"  æ€»é—®é¢˜æ•°: {report['issues_summary']['total_issues']}")
        for severity, count in report['issues_summary']['by_severity'].items():
            if count > 0:
                print(f"  {severity}: {count} ä¸ª")
        
        if report['detailed_issues']:
            print(f"\nğŸ” è¯¦ç»†é—®é¢˜åˆ—è¡¨:")
            for i, issue in enumerate(report['detailed_issues'][:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  {i}. [{issue['severity']}] {issue['category']}")
                print(f"     {issue['description']}")
                print(f"     æ–‡ä»¶: {issue['file']}")
                if issue['suggestion']:
                    print(f"     å»ºè®®: {issue['suggestion']}")
                print()
        
        print(f"ğŸ¯ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"  {i}. {rec}")
        
        print(f"{'='*70}\n")
    
    def save_evaluation_report(self, filepath: str):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
        """
        if not self.evaluation_results:
            print("é”™è¯¯: æ²¡æœ‰å¯ä¿å­˜çš„è¯„ä¼°ç»“æœ")
            return
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.evaluation_results, f, ensure_ascii=False, indent=2)
        
        print(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä»£ç è´¨é‡è¯„ä¼°å™¨')
    parser.add_argument('--project', '-p', 
                       default=os.path.dirname(os.path.abspath(__file__)), 
                       help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('--output', '-o', help='ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = TestEvaluator(args.project)
    
    # æ‰§è¡Œè¯„ä¼°
    report = evaluator.evaluate_project()
    
    # æ˜¾ç¤ºç»“æœ
    if not args.quiet:
        evaluator.print_evaluation_report()
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        evaluator.save_evaluation_report(args.output)
    
    # æ ¹æ®è´¨é‡ç­‰çº§è¿”å›é€€å‡ºç 
    quality_level = report['quality_level']
    if quality_level in ['ä¼˜ç§€', 'è‰¯å¥½']:
        exit_code = 0
    elif quality_level == 'ä¸€èˆ¬':
        exit_code = 1
    else:
        exit_code = 2
    
    return exit_code


if __name__ == '__main__':
    import sys
    sys.exit(main())
